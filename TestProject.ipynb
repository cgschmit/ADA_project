{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give the average sentiment PER CANTON or per city (more precise is imposible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitter=pd.read_json(\"harvest3r_twitter_data_12-01_0.json\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Merger (on the server)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "files=[name for name in os.listdir('.') if os.path.isfile(name)];\n",
    "AllTwitter=[];\n",
    "AllInstagram=[];\n",
    "AllNews=[];\n",
    "a=0;\n",
    "#Add Loop \"for\" to go in different directory (january to october)\n",
    "for i in ['january','february','march','april','mai','june','july','august','september','october']:\n",
    "    files=[name for name in os.listdir('./'+i+\"/\") if os.path.isfile(name)]; #Change the path accordingly\n",
    "    for i in files:\n",
    "        if files[a].find('twitter')>0:\n",
    "            try:\n",
    "                twitter=pd.read_json(i);\n",
    "                AllTwitter.append(twitter);\n",
    "            except ValueError:\n",
    "                print (\"JSON object issue\");\n",
    "        elif files[a].find('instagram')>0:\n",
    "            try:\n",
    "                instagram=pd.read_json(i);\n",
    "                AllInstagram.append(instagram);\n",
    "            except ValueError:\n",
    "                print (\"JSON object issue\");\n",
    "        elif files[a].find('news')>0:\n",
    "            try:\n",
    "                news=pd.read_json(i);\n",
    "                AllNews.append(news);\n",
    "            except ValueError:\n",
    "                print (\"JSON object issue\");\n",
    "        a=a+1;\n",
    "\n",
    "#To transform into DataFrame\n",
    "AllTwitter=pd.concat(AllTwitter, ignore_index=True); #To transform into DataFrame\n",
    "AllInstagram=pd.concat(AllInstagram, ignore_index=True);\n",
    "AllNews=pd.concat(AllNews, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File merger (locally)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON object issue\n"
     ]
    }
   ],
   "source": [
    "import os, os.path\n",
    "files=[name for name in os.listdir('.') if os.path.isfile(name)];\n",
    "AllTwitter=[];\n",
    "AllInstagram=[];\n",
    "AllNews=[];\n",
    "a=0;\n",
    "\n",
    "files=[name for name in os.listdir('.') if os.path.isfile(name)]; \n",
    "for i in files:\n",
    "    if files[a].find('twitter')>0:\n",
    "        try:\n",
    "            twitter=pd.read_json(i);\n",
    "            AllTwitter.append(twitter);\n",
    "        except ValueError:\n",
    "            print (\"JSON object issue\");\n",
    "    elif files[a].find('instagram')>0:\n",
    "        try:\n",
    "            instagram=pd.read_json(i);\n",
    "            AllInstagram.append(instagram);\n",
    "        except ValueError:\n",
    "            print (\"JSON object issue\");\n",
    "    elif files[a].find('news')>0:\n",
    "        try:\n",
    "            news=pd.read_json(i);\n",
    "            AllNews.append(news);\n",
    "        except ValueError:\n",
    "            print (\"JSON object issue\");\n",
    "    a=a+1;\n",
    "\n",
    "#To transform into DataFrame\n",
    "AllTwitter=pd.concat(AllTwitter, ignore_index=True); #To transform into DataFrame\n",
    "AllInstagram=pd.concat(AllInstagram, ignore_index=True);\n",
    "AllNews=pd.concat(AllNews, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to separate row with sentiment to row without + Write in positive.txt and negative.txt + delete stop_words (useless ones)\n",
    "\n",
    "\n",
    "# I cannot concatenate, don't know why !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def deleteContent(pfile):\n",
    "    pfile.seek(0)\n",
    "    pfile.truncate()\n",
    "\n",
    "additional_stopwords=['.',',','/','(',')','[',']',':','@','\\'','?','!','\\\"',';','&']\n",
    "TwitterWithPos=[]\n",
    "TwitterWithNeg=[]\n",
    "TwitterWithout=[]\n",
    "fPos = open('positive.txt', 'r+')\n",
    "fNeg = open('negative.txt', 'r+')\n",
    "fNeu = open('neutral.txt', 'r+')\n",
    "#English\n",
    "#fPosEN = open('positive.txt', 'r+')\n",
    "#fNegEN = open('negative.txt', 'r+')\n",
    "#fNeuEN = open('neutral.txt', 'r+')\n",
    "#French\n",
    "#fPosFR = open('positive.txt', 'r+')\n",
    "#fNegFR = open('negative.txt', 'r+')\n",
    "#fNeuFR = open('neutral.txt', 'r+')\n",
    "#German\n",
    "#fPosDE = open('positive.txt', 'r+')\n",
    "#fNegDE = open('negative.txt', 'r+')\n",
    "#fNeuDE = open('neutral.txt', 'r+')\n",
    "#Italian\n",
    "#fPosIT = open('positive.txt', 'r+')\n",
    "#fNegIT = open('negative.txt', 'r+')\n",
    "#fNeuIT = open('neutral.txt', 'r+')\n",
    "\n",
    "deleteContent(fPos)\n",
    "deleteContent(fNeg)\n",
    "deleteContent(fNeu)\n",
    "twitter['language'] = pd.Series(np.zeros(len(twitter)));\n",
    "\n",
    "for i in range(0,400): #TO CHANGE !!! len(twitter)\n",
    "    sentence=twitter.iloc[i][\"_source\"].get('main');\n",
    "    language='None';\n",
    "    try:\n",
    "        language=detect(sentence); \n",
    "    except:\n",
    "        e=1;\n",
    "    twitter.ix[i,'language']=language;\n",
    "    #FIND the language of the current Tweet\n",
    "    if language=='en' or language=='de' or language=='it' or language=='fr': \n",
    "        langDict = {'en': 'english', 'de': 'german', 'it':'italian', 'fr':'french'} \n",
    "        stop_words = set(stopwords.words(langDict[language])) \n",
    "        word_tokens = word_tokenize(sentence)\n",
    "        filtered_sentence = []\n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words and w not in additional_stopwords and len(w)>=3:\n",
    "                filtered_sentence.append(w);\n",
    "    #FIND Tweets without sentiment and place it into a text file per language\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        TwitterWithout.append(twitter.iloc[[i]])\n",
    "        try:\n",
    "            filtered_sentence=' '.join(filtered_sentence)\n",
    "        except ValueError:\n",
    "            print('One-word Tweet')  \n",
    "        fNeu.write(str(filtered_sentence)+\"\\n\");\n",
    "    #FIND Tweets with sentiment and place it into a text file per language\n",
    "    else:\n",
    "        #FIND Tweet with POSITIVE sentiment\n",
    "        if twitter._source[i].get('sentiment')=='POSITIVE':\n",
    "            TwitterWithPos.append(twitter.iloc[[i]])\n",
    "            try:\n",
    "                filtered_sentence=' '.join(filtered_sentence)\n",
    "            except ValueError:\n",
    "                print('One-word Tweet') \n",
    "            fPos.write(str(filtered_sentence)+\"\\n\");\n",
    "        #FIND Tweet with NEGATIVE sentiment\n",
    "        elif twitter._source[i].get('sentiment')=='NEGATIVE':\n",
    "            TwitterWithNeg.append(twitter.iloc[[i]])\n",
    "            try:\n",
    "                filtered_sentence=' '.join(filtered_sentence)\n",
    "            except ValueError:\n",
    "                print('One-word Tweet') \n",
    "            fNeg.write(str(filtered_sentence)+\"\\n\");\n",
    "\n",
    "#To transform into DataFrame       \n",
    "TwitterWithPos=pd.concat(TwitterWithPos, ignore_index=True)\n",
    "TwitterWithNeg=pd.concat(TwitterWithNeg, ignore_index=True)\n",
    "TwitterWithout=pd.concat(TwitterWithout, ignore_index=True)\n",
    "\n",
    "fPos.close()\n",
    "fNeg.close()\n",
    "fNeu.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Last work to classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last problem still occuring:\n",
    "\n",
    "1) I have no clue how to get all the word features in \"extract_features\" function. Problem linked to the fact we're using: nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_train) \n",
    "    \n",
    "    nltk.classify.apply_features(function, toks)\n",
    "    \n",
    "    [function(tok) for tok in toks]\n",
    "    \n",
    "2) Problem to compute the accuracy. It worked at some point but could'nt fix it back with all the changes I added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming Words would be nice feat to add, but need to get one for each language..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A ELE probability distribution must have at least one bin.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-5717c43673cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;31m#    training_set_en.append(extract_features(all_filtered_sentence_en_train[i],word_features_en))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0mtraining_set_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_filtered_sentence_en_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m \u001b[0mclassifier_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;31m#word_features_fr = get_word_features(get_words_in_tweets(all_filtered_sentence_fr))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cyrilschmitt/anaconda/lib/python3.5/site-packages/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# Create the P(label) distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mlabel_probdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_freqdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Create the P(fval|label, fname) distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cyrilschmitt/anaconda/lib/python3.5/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, freqdist, bins)\u001b[0m\n\u001b[1;32m    848\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspecified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfreqdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \"\"\"\n\u001b[0;32m--> 850\u001b[0;31m         \u001b[0mLidstoneProbDist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cyrilschmitt/anaconda/lib/python3.5/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, freqdist, gamma, bins)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             raise ValueError('A %s probability distribution ' % name +\n\u001b[0;32m--> 730\u001b[0;31m                              'must have at least one bin.')\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A ELE probability distribution must have at least one bin."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from nltk.stem import PorterStemmer\n",
    "import time\n",
    "import string\n",
    "start_time = time.time()\n",
    "\n",
    "### CREATION STOP WORDS !!!   \n",
    "with open('stopwords/de') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_de = []\n",
    "for i in mylist:\n",
    "    stopwords_de.append(i)\n",
    "    \n",
    "with open('stopwords/en') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_en = []\n",
    "for i in mylist:\n",
    "    stopwords_en.append(i)\n",
    "    \n",
    "with open('stopwords/it') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_it = []\n",
    "for i in mylist:\n",
    "    stopwords_it.append(i)\n",
    "    \n",
    "with open('stopwords/fr') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_fr = []\n",
    "for i in mylist:\n",
    "    stopwords_fr.append(i)\n",
    "### FIN CREATION STOPWORDS\n",
    "    \n",
    "def filtering(stop_words, index):\n",
    "    sentence=twitterAllWith.iloc[index][\"_source\"].get('main');\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    words_filtered=[x.lower() for x in word_tokens if x not in stop_words if x not in string.punctuation if len(x)>=3 ]\n",
    "    #COULD BE ADDED IF NEEDED : if x.find('youtu.be')==-1 if x.find('twitter.com')==-1 if x.find('www')==-1 if x.find('.com')==-1\n",
    "    return words_filtered\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in get_word_features(document):\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "\"\"\"\n",
    "TwitterWithPos=[]\n",
    "TwitterWithNeg=[]\n",
    "TwitterWithNeu=[]\n",
    "TwitterWithout=[]\n",
    "twitterAllWith=[]\n",
    "all_filtered_sentence_en=[]\n",
    "all_filtered_sentence_de=[]\n",
    "all_filtered_sentence_fr=[]\n",
    "all_filtered_sentence_it=[]\n",
    "\n",
    "\n",
    "#DISSOCIATE ROW WITH/WITHOUT SENTIMENT     \n",
    "#FIND Tweets without sentiment and place it into a text file per language\n",
    "\n",
    "for i in range(0,len(twitter)):\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        TwitterWithout.append(twitter.iloc[[i]])\n",
    "    else:\n",
    "        #FIND Tweet with POSITIVE sentiment\n",
    "        if twitter._source[i].get('sentiment')=='POSITIVE':\n",
    "            TwitterWithPos.append(twitter.iloc[[i]]) \n",
    "        #FIND Tweet with NEGATIVE sentiment\n",
    "        elif twitter._source[i].get('sentiment')=='NEGATIVE':\n",
    "            TwitterWithNeg.append(twitter.iloc[[i]])\n",
    "        #FIND Tweet with NEUTRAL sentiment\n",
    "        elif twitter._source[i].get('sentiment')=='NEUTRAL':\n",
    "            TwitterWithNeu.append(twitter.iloc[[i]])\n",
    "\n",
    "#To transform into DataFrame       \n",
    "TwitterWithPos=pd.concat(TwitterWithPos, ignore_index=True)\n",
    "TwitterWithNeg=pd.concat(TwitterWithNeg, ignore_index=True)\n",
    "TwitterWithout=pd.concat(TwitterWithout, ignore_index=True)\n",
    "\n",
    "frames = [TwitterWithPos, TwitterWithNeg,TwitterWithNeu]\n",
    "twitterAllWith = pd.concat(frames)\n",
    "twitterAllWith = twitterAllWith.sample(frac=1).reset_index(drop=True) #Shuffle DF\n",
    "\"\"\"  \n",
    "TwitterWith=twitter\n",
    "for i in range(0,len(twitter)):\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        TwitterWith.drop(TwitterWith.index[[i]])\n",
    "        \n",
    "TwitterWith = TwitterWith.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "twitter['language'] = pd.Series(np.zeros(len(twitter)));\n",
    "\n",
    "for i in range(0,len(twitterAllWith)): #TO CHANGE !!! len(twitterAllWith)\n",
    "\n",
    "    #ADD COLUMN TO ADD THE LANGUAGE\n",
    "    sentence=twitterAllWith.iloc[i][\"_source\"].get('main')\n",
    "    language='None';\n",
    "    try:\n",
    "        language=detect(sentence)\n",
    "    except:\n",
    "        e=1;\n",
    "        \n",
    "    twitterAllWith.ix[i,'language']=language\n",
    "\n",
    "    if language=='en':\n",
    "        words_filtered=filtering(stopwords_en,i)\n",
    "        all_filtered_sentence_en.append((words_filtered,twitterAllWith._source[i].get('sentiment'))) \n",
    "    elif language=='fr':\n",
    "        words_filtered=filtering(stopwords_fr,i)\n",
    "        all_filtered_sentence_fr.append((words_filtered,twitterAllWith._source[i].get('sentiment')))\n",
    "    elif language=='de':\n",
    "        words_filtered=filtering(stopwords_de,i)\n",
    "        all_filtered_sentence_de.append((words_filtered,twitterAllWith._source[i].get('sentiment')))\n",
    "    elif language=='it':\n",
    "        words_filtered=filtering(stopwords_it,i)\n",
    "        all_filtered_sentence_it.append((words_filtered,twitterAllWith._source[i].get('sentiment')))                        \n",
    "\n",
    "\n",
    "#CLASSIFYING\n",
    "#word_features_en = get_word_features(get_words_in_tweets(all_filtered_sentence_en))\n",
    "all_filtered_sentence_en_train=all_filtered_sentence_en[:round(0.8*len(all_filtered_sentence_en))]\n",
    "#for i in range(0,len(all_filtered_sentence_en_train)):\n",
    "#    training_set_en.append(extract_features(all_filtered_sentence_en_train[i],word_features_en))\n",
    "training_set_en = nltk.classify.apply_features(extract_features, all_filtered_sentence_en_train)\n",
    "classifier_en = nltk.NaiveBayesClassifier.train(training_set_en)\n",
    "\n",
    "#word_features_fr = get_word_features(get_words_in_tweets(all_filtered_sentence_fr))\n",
    "all_filtered_sentence_fr_train=all_filtered_sentence_fr[:round(0.8*len(all_filtered_sentence_fr))]\n",
    "training_set_fr = nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_train)\n",
    "classifier_fr = nltk.NaiveBayesClassifier.train(training_set_fr)\n",
    "\n",
    "#word_features_de = get_word_features(get_words_in_tweets(all_filtered_sentence_de))\n",
    "all_filtered_sentence_de_train=all_filtered_sentence_de[:round(0.8*len(all_filtered_sentence_de))]\n",
    "training_set_de = nltk.classify.apply_features(extract_features, all_filtered_sentence_de_train)\n",
    "classifier_de = nltk.NaiveBayesClassifier.train(training_set_de)\n",
    "\n",
    "#word_features_it = get_word_features(get_words_in_tweets(all_filtered_sentence_it))\n",
    "all_filtered_sentence_it_train=all_filtered_sentence_it[:round(0.8*len(all_filtered_sentence_it))]\n",
    "training_set_it = nltk.classify.apply_features(extract_features, all_filtered_sentence_it_train)\n",
    "classifier_it = nltk.NaiveBayesClassifier.train(training_set_it)\n",
    "\n",
    "\n",
    "#PRINT RESULTS\n",
    "all_filtered_sentence_en_test=all_filtered_sentence_en[round(0.8*len(all_filtered_sentence_en)):]\n",
    "testing_set_en = nltk.classify.apply_features(extract_features, all_filtered_sentence_en_test)\n",
    "print(nltk.classify.accuracy(classifier_en, testing_set_en)*100)\n",
    "\n",
    "all_filtered_sentence_fr_test=all_filtered_sentence_fr[round(0.8*len(all_filtered_sentence_fr)):]\n",
    "testing_set_fr = nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_test)\n",
    "print(nltk.classify.accuracy(classifier_fr, testing_set_fr)*100)\n",
    "\n",
    "all_filtered_sentence_de_test=all_filtered_sentence_de[round(0.8*len(all_filtered_sentence_de)):]\n",
    "testing_set_de = nltk.classify.apply_features(extract_features, all_filtered_sentence_de_test)\n",
    "print(nltk.classify.accuracy(classifier_de, testing_set_de)*100)\n",
    "\n",
    "all_filtered_sentence_it_test=all_filtered_sentence_it[round(0.8*len(all_filtered_sentence_it)):]\n",
    "testing_set_it = nltk.classify.apply_features(extract_features, all_filtered_sentence_it_test)\n",
    "print(nltk.classify.accuracy(classifier_it, testing_set_it)*100)\n",
    "\n",
    "####\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from nltk.stem import PorterStemmer\n",
    "import time\n",
    "import string\n",
    "start_time = time.time()\n",
    "\n",
    "### CREATION STOP WORDS !!!   \n",
    "with open('stopwords/de') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_de = []\n",
    "for i in mylist:\n",
    "    stopwords_de.append(i)\n",
    "    \n",
    "with open('stopwords/en') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_en = []\n",
    "for i in mylist:\n",
    "    stopwords_en.append(i)\n",
    "    \n",
    "with open('stopwords/it') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_it = []\n",
    "for i in mylist:\n",
    "    stopwords_it.append(i)\n",
    "    \n",
    "with open('stopwords/fr') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_fr = []\n",
    "for i in mylist:\n",
    "    stopwords_fr.append(i)\n",
    "### FIN CREATION STOPWORDS\n",
    "    \n",
    "def filtering(stop_words, index):\n",
    "    sentence=twitterAllWith.iloc[index][\"_source\"].get('main');\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    words_filtered=[x.lower() for x in word_tokens if x not in stop_words if x not in string.punctuation if len(x)>=3 ]\n",
    "    #COULD BE ADDED IF NEEDED : if x.find('youtu.be')==-1 if x.find('twitter.com')==-1 if x.find('www')==-1 if x.find('.com')==-1\n",
    "    return words_filtered\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in get_word_features(document):\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "\"\"\"\n",
    "TwitterWithPos=[]\n",
    "TwitterWithNeg=[]\n",
    "TwitterWithNeu=[]\n",
    "TwitterWithout=[]\n",
    "twitterAllWith=[]\n",
    "all_filtered_sentence_en=[]\n",
    "all_filtered_sentence_de=[]\n",
    "all_filtered_sentence_fr=[]\n",
    "all_filtered_sentence_it=[]\n",
    "\n",
    "\n",
    "#DISSOCIATE ROW WITH/WITHOUT SENTIMENT     \n",
    "#FIND Tweets without sentiment and place it into a text file per language\n",
    "\n",
    "for i in range(0,len(twitter)):\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        TwitterWithout.append(twitter.iloc[[i]])\n",
    "    else:\n",
    "        #FIND Tweet with POSITIVE sentiment\n",
    "        if twitter._source[i].get('sentiment')=='POSITIVE':\n",
    "            TwitterWithPos.append(twitter.iloc[[i]]) \n",
    "        #FIND Tweet with NEGATIVE sentiment\n",
    "        elif twitter._source[i].get('sentiment')=='NEGATIVE':\n",
    "            TwitterWithNeg.append(twitter.iloc[[i]])\n",
    "        #FIND Tweet with NEUTRAL sentiment\n",
    "        elif twitter._source[i].get('sentiment')=='NEUTRAL':\n",
    "            TwitterWithNeu.append(twitter.iloc[[i]])\n",
    "\n",
    "#To transform into DataFrame       \n",
    "TwitterWithPos=pd.concat(TwitterWithPos, ignore_index=True)\n",
    "TwitterWithNeg=pd.concat(TwitterWithNeg, ignore_index=True)\n",
    "TwitterWithout=pd.concat(TwitterWithout, ignore_index=True)\n",
    "\n",
    "frames = [TwitterWithPos, TwitterWithNeg,TwitterWithNeu]\n",
    "twitterAllWith = pd.concat(frames)\n",
    "twitterAllWith = twitterAllWith.sample(frac=1).reset_index(drop=True) #Shuffle DF\n",
    "\"\"\"  \n",
    "TwitterWith=twitter\n",
    "for i in range(0,len(twitter)):\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        TwitterWith.drop(TwitterWith.index[[i]])\n",
    "        \n",
    "TwitterWith = TwitterWith.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "twitter['language'] = pd.Series(np.zeros(len(twitter)));\n",
    "\n",
    "for i in range(0,len(twitterAllWith)): #TO CHANGE !!! len(twitterAllWith)\n",
    "\n",
    "    #ADD COLUMN TO ADD THE LANGUAGE\n",
    "    sentence=twitterAllWith.iloc[i][\"_source\"].get('main')\n",
    "    language='None';\n",
    "    try:\n",
    "        language=detect(sentence)\n",
    "    except:\n",
    "        e=1;\n",
    "        \n",
    "    twitterAllWith.ix[i,'language']=language\n",
    "\n",
    "    if language=='en':\n",
    "        words_filtered=filtering(stopwords_en,i)\n",
    "        all_filtered_sentence_en.append((words_filtered,twitterAllWith._source[i].get('sentiment'))) \n",
    "    elif language=='fr':\n",
    "        words_filtered=filtering(stopwords_fr,i)\n",
    "        all_filtered_sentence_fr.append((words_filtered,twitterAllWith._source[i].get('sentiment')))\n",
    "    elif language=='de':\n",
    "        words_filtered=filtering(stopwords_de,i)\n",
    "        all_filtered_sentence_de.append((words_filtered,twitterAllWith._source[i].get('sentiment')))\n",
    "    elif language=='it':\n",
    "        words_filtered=filtering(stopwords_it,i)\n",
    "        all_filtered_sentence_it.append((words_filtered,twitterAllWith._source[i].get('sentiment')))                        \n",
    "\n",
    "\n",
    "#CLASSIFYING\n",
    "#word_features_en = get_word_features(get_words_in_tweets(all_filtered_sentence_en))\n",
    "all_filtered_sentence_en_train=all_filtered_sentence_en[:round(0.8*len(all_filtered_sentence_en))]\n",
    "#for i in range(0,len(all_filtered_sentence_en_train)):\n",
    "#    training_set_en.append(extract_features(all_filtered_sentence_en_train[i],word_features_en))\n",
    "training_set_en = nltk.classify.apply_features(extract_features, all_filtered_sentence_en_train)\n",
    "classifier_en = nltk.NaiveBayesClassifier.train(training_set_en)\n",
    "\n",
    "#word_features_fr = get_word_features(get_words_in_tweets(all_filtered_sentence_fr))\n",
    "all_filtered_sentence_fr_train=all_filtered_sentence_fr[:round(0.8*len(all_filtered_sentence_fr))]\n",
    "training_set_fr = nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_train)\n",
    "classifier_fr = nltk.NaiveBayesClassifier.train(training_set_fr)\n",
    "\n",
    "#word_features_de = get_word_features(get_words_in_tweets(all_filtered_sentence_de))\n",
    "all_filtered_sentence_de_train=all_filtered_sentence_de[:round(0.8*len(all_filtered_sentence_de))]\n",
    "training_set_de = nltk.classify.apply_features(extract_features, all_filtered_sentence_de_train)\n",
    "classifier_de = nltk.NaiveBayesClassifier.train(training_set_de)\n",
    "\n",
    "#word_features_it = get_word_features(get_words_in_tweets(all_filtered_sentence_it))\n",
    "all_filtered_sentence_it_train=all_filtered_sentence_it[:round(0.8*len(all_filtered_sentence_it))]\n",
    "training_set_it = nltk.classify.apply_features(extract_features, all_filtered_sentence_it_train)\n",
    "classifier_it = nltk.NaiveBayesClassifier.train(training_set_it)\n",
    "\n",
    "\n",
    "#PRINT RESULTS\n",
    "all_filtered_sentence_en_test=all_filtered_sentence_en[round(0.8*len(all_filtered_sentence_en)):]\n",
    "testing_set_en = nltk.classify.apply_features(extract_features, all_filtered_sentence_en_test)\n",
    "print(nltk.classify.accuracy(classifier_en, testing_set_en)*100)\n",
    "\n",
    "all_filtered_sentence_fr_test=all_filtered_sentence_fr[round(0.8*len(all_filtered_sentence_fr)):]\n",
    "testing_set_fr = nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_test)\n",
    "print(nltk.classify.accuracy(classifier_fr, testing_set_fr)*100)\n",
    "\n",
    "all_filtered_sentence_de_test=all_filtered_sentence_de[round(0.8*len(all_filtered_sentence_de)):]\n",
    "testing_set_de = nltk.classify.apply_features(extract_features, all_filtered_sentence_de_test)\n",
    "print(nltk.classify.accuracy(classifier_de, testing_set_de)*100)\n",
    "\n",
    "all_filtered_sentence_it_test=all_filtered_sentence_it[round(0.8*len(all_filtered_sentence_it)):]\n",
    "testing_set_it = nltk.classify.apply_features(extract_features, all_filtered_sentence_it_test)\n",
    "print(nltk.classify.accuracy(classifier_it, testing_set_it)*100)\n",
    "\n",
    "####\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "18\n",
      "39\n",
      "40\n",
      "47\n",
      "50\n",
      "59\n",
      "75\n",
      "80\n",
      "83\n",
      "84\n",
      "87\n",
      "90\n",
      "106\n",
      "116\n",
      "118\n",
      "126\n",
      "132\n",
      "167\n",
      "176\n",
      "224\n",
      "226\n",
      "239\n",
      "249\n",
      "280\n",
      "291\n",
      "308\n",
      "320\n",
      "326\n",
      "329\n",
      "331\n",
      "332\n",
      "343\n",
      "351\n",
      "366\n",
      "375\n",
      "383\n",
      "404\n",
      "414\n",
      "417\n",
      "444\n",
      "445\n",
      "455\n",
      "463\n",
      "480\n",
      "484\n",
      "486\n",
      "496\n",
      "534\n",
      "538\n",
      "543\n",
      "549\n",
      "550\n",
      "551\n",
      "575\n",
      "596\n",
      "597\n",
      "630\n",
      "649\n",
      "661\n",
      "678\n",
      "692\n",
      "694\n",
      "700\n",
      "711\n",
      "712\n",
      "714\n",
      "726\n",
      "740\n",
      "785\n",
      "794\n",
      "811\n",
      "814\n",
      "821\n",
      "829\n",
      "850\n",
      "888\n",
      "892\n",
      "906\n",
      "909\n",
      "911\n",
      "923\n",
      "933\n",
      "950\n",
      "980\n",
      "1021\n",
      "1096\n",
      "1117\n",
      "1133\n",
      "1154\n",
      "1173\n",
      "1194\n",
      "1241\n",
      "1278\n",
      "1344\n",
      "1375\n",
      "1473\n",
      "1499\n",
      "1511\n",
      "1543\n",
      "1696\n",
      "1751\n",
      "1758\n",
      "1767\n",
      "1777\n",
      "1792\n",
      "1843\n",
      "1894\n",
      "1906\n",
      "1924\n",
      "1954\n",
      "1980\n",
      "1991\n",
      "2029\n",
      "2067\n",
      "2079\n",
      "2083\n",
      "2088\n",
      "2095\n",
      "2111\n",
      "2119\n",
      "2134\n",
      "2137\n",
      "2139\n",
      "2143\n",
      "2148\n",
      "2150\n",
      "2153\n",
      "2154\n",
      "2159\n",
      "2167\n",
      "2172\n",
      "2177\n",
      "2194\n",
      "2207\n",
      "2211\n",
      "2212\n",
      "2214\n",
      "2244\n",
      "2247\n",
      "2253\n",
      "2260\n",
      "2281\n",
      "2282\n",
      "2290\n",
      "2306\n",
      "2309\n",
      "2317\n",
      "2369\n",
      "2371\n",
      "2375\n",
      "2377\n",
      "2381\n",
      "2401\n",
      "2425\n",
      "2483\n",
      "2510\n",
      "2516\n",
      "2557\n",
      "2571\n",
      "2734\n",
      "2775\n",
      "2811\n",
      "2816\n",
      "2941\n",
      "2946\n",
      "2958\n",
      "3023\n",
      "3027\n",
      "3031\n",
      "3035\n",
      "3090\n",
      "3117\n",
      "3131\n",
      "3150\n",
      "3257\n",
      "3345\n",
      "3346\n",
      "3421\n",
      "3426\n",
      "3446\n",
      "3449\n",
      "3557\n",
      "3568\n",
      "3573\n",
      "3596\n",
      "3611\n",
      "3621\n",
      "3695\n",
      "3785\n",
      "3809\n",
      "3820\n",
      "3825\n",
      "3835\n",
      "3856\n",
      "3865\n",
      "3904\n",
      "3934\n",
      "3987\n",
      "4086\n",
      "4105\n",
      "4124\n",
      "4125\n",
      "4127\n",
      "4200\n",
      "4223\n",
      "4228\n",
      "4315\n",
      "4435\n",
      "4438\n",
      "4443\n",
      "4554\n",
      "4563\n",
      "4596\n",
      "4640\n",
      "4683\n",
      "4701\n",
      "4703\n",
      "4745\n",
      "4813\n",
      "4814\n",
      "4817\n",
      "4832\n",
      "4869\n",
      "4970\n",
      "5022\n",
      "5056\n",
      "5082\n",
      "5084\n",
      "5096\n",
      "5139\n",
      "5145\n",
      "5156\n",
      "5183\n",
      "5230\n",
      "5238\n",
      "5255\n",
      "5266\n",
      "5269\n",
      "5295\n",
      "5324\n",
      "5327\n",
      "5334\n",
      "5471\n",
      "5487\n",
      "5491\n",
      "5505\n",
      "5685\n",
      "5720\n",
      "5764\n",
      "5770\n",
      "5774\n",
      "5775\n",
      "5779\n",
      "5860\n",
      "5863\n",
      "5890\n",
      "5917\n",
      "5947\n",
      "5997\n",
      "6007\n",
      "6013\n",
      "6017\n",
      "6074\n",
      "6075\n",
      "6084\n",
      "6090\n",
      "6093\n",
      "6119\n",
      "6126\n",
      "6127\n",
      "6130\n",
      "6137\n",
      "6143\n",
      "6146\n",
      "6151\n",
      "6203\n",
      "6214\n",
      "6268\n",
      "6310\n",
      "6313\n",
      "6317\n",
      "6365\n",
      "6430\n",
      "6431\n",
      "6496\n",
      "6531\n",
      "6551\n",
      "6553\n",
      "6567\n",
      "6572\n",
      "6588\n",
      "6602\n",
      "6716\n",
      "6739\n",
      "6744\n",
      "6755\n",
      "6772\n",
      "6793\n",
      "6797\n",
      "6816\n",
      "6863\n",
      "6888\n",
      "6899\n",
      "6915\n",
      "6956\n",
      "6969\n",
      "7026\n",
      "7065\n",
      "7104\n",
      "7113\n",
      "7176\n",
      "7184\n",
      "7185\n",
      "7259\n",
      "7292\n",
      "7297\n",
      "7302\n",
      "7321\n",
      "7323\n",
      "7329\n",
      "7334\n",
      "7338\n",
      "7346\n",
      "7351\n",
      "7363\n",
      "7378\n",
      "7438\n",
      "7533\n",
      "7542\n",
      "7557\n",
      "7563\n",
      "7571\n",
      "7584\n",
      "7596\n",
      "7612\n",
      "7613\n",
      "7665\n",
      "7679\n",
      "7693\n",
      "7797\n",
      "7817\n",
      "7826\n",
      "7849\n",
      "7850\n",
      "7874\n",
      "7875\n",
      "7893\n",
      "7943\n",
      "7978\n",
      "7987\n",
      "7988\n",
      "7994\n",
      "8009\n",
      "8076\n",
      "8133\n",
      "8135\n",
      "8249\n",
      "8252\n",
      "8283\n",
      "8343\n",
      "8357\n",
      "8385\n",
      "8430\n",
      "8460\n",
      "8471\n",
      "8487\n",
      "8540\n",
      "8595\n",
      "8598\n",
      "8600\n",
      "8606\n",
      "8612\n",
      "8625\n",
      "8627\n",
      "8644\n",
      "8732\n",
      "8737\n",
      "8762\n",
      "8783\n",
      "8786\n",
      "8803\n",
      "8805\n",
      "8825\n",
      "8826\n",
      "8828\n",
      "8841\n",
      "8853\n",
      "8854\n",
      "8868\n",
      "8875\n",
      "8901\n",
      "8906\n",
      "8965\n",
      "8992\n",
      "8997\n",
      "9016\n",
      "9021\n",
      "9030\n",
      "9064\n",
      "9175\n",
      "9181\n",
      "9195\n",
      "9213\n",
      "9252\n",
      "9254\n",
      "9296\n",
      "9309\n",
      "9398\n",
      "9405\n",
      "9421\n",
      "9492\n",
      "9503\n",
      "9507\n",
      "9544\n",
      "9563\n",
      "9596\n",
      "9624\n",
      "9690\n",
      "9699\n",
      "9701\n",
      "9719\n",
      "9746\n",
      "9755\n",
      "9771\n",
      "9774\n",
      "9787\n",
      "9799\n",
      "9808\n",
      "9821\n",
      "9824\n",
      "9850\n",
      "9865\n",
      "9898\n",
      "9900\n",
      "9903\n",
      "9905\n",
      "9953\n",
      "9970\n",
      "9991\n",
      "9994\n",
      "10002\n",
      "10005\n",
      "10036\n",
      "10044\n",
      "10049\n",
      "10077\n",
      "10079\n",
      "10088\n",
      "10110\n",
      "10113\n",
      "10127\n",
      "10157\n",
      "10181\n",
      "10224\n",
      "10250\n",
      "10255\n",
      "10260\n",
      "10263\n",
      "10289\n",
      "10303\n",
      "10313\n",
      "10329\n",
      "10345\n",
      "10357\n",
      "10362\n",
      "10364\n",
      "10391\n",
      "10393\n",
      "10396\n",
      "10434\n",
      "10436\n",
      "10447\n",
      "10448\n",
      "10449\n",
      "10491\n",
      "10497\n",
      "10527\n",
      "10537\n",
      "10545\n",
      "10609\n",
      "10613\n",
      "10619\n",
      "10625\n",
      "10651\n",
      "10654\n",
      "10662\n",
      "10701\n",
      "10707\n",
      "10733\n",
      "10751\n",
      "10779\n",
      "10792\n",
      "10811\n",
      "10845\n",
      "10870\n",
      "10873\n",
      "10876\n",
      "10902\n",
      "10921\n",
      "10935\n",
      "10936\n",
      "10969\n",
      "10976\n",
      "10987\n",
      "11022\n",
      "11050\n",
      "11072\n",
      "11093\n",
      "11130\n",
      "11144\n",
      "11145\n",
      "11169\n",
      "11175\n",
      "11179\n",
      "11181\n",
      "11187\n",
      "11210\n",
      "11217\n",
      "11258\n",
      "11274\n",
      "11292\n",
      "11305\n",
      "11306\n",
      "11334\n",
      "11343\n",
      "11348\n",
      "11350\n",
      "11352\n",
      "11354\n",
      "11371\n",
      "11404\n",
      "11440\n",
      "11451\n",
      "11453\n",
      "11479\n",
      "11525\n",
      "11587\n",
      "11604\n",
      "11636\n",
      "11676\n",
      "11756\n",
      "11758\n",
      "11773\n",
      "11801\n",
      "11802\n",
      "11805\n",
      "11859\n",
      "11881\n",
      "11888\n",
      "11907\n",
      "11940\n",
      "11968\n",
      "11995\n",
      "11999\n",
      "12004\n",
      "12010\n",
      "12014\n",
      "12016\n",
      "12053\n",
      "12059\n",
      "12066\n",
      "12096\n",
      "12110\n",
      "12121\n",
      "12123\n",
      "12146\n",
      "12155\n",
      "12159\n",
      "12181\n",
      "12192\n",
      "12209\n",
      "12225\n",
      "12273\n",
      "12279\n",
      "12287\n",
      "12304\n",
      "12340\n",
      "12375\n",
      "12383\n",
      "12384\n",
      "12410\n",
      "12417\n",
      "12426\n",
      "12525\n",
      "12565\n",
      "12571\n",
      "12574\n",
      "12689\n",
      "12724\n",
      "12744\n",
      "12768\n",
      "12828\n",
      "12830\n",
      "12843\n",
      "12890\n",
      "12891\n",
      "12892\n",
      "12928\n",
      "12991\n",
      "13068\n",
      "13123\n",
      "13218\n",
      "13247\n",
      "13259\n",
      "13266\n",
      "13313\n",
      "13342\n",
      "13357\n",
      "13361\n",
      "13404\n",
      "13427\n",
      "13449\n",
      "13455\n",
      "13458\n",
      "13472\n",
      "13485\n",
      "13492\n",
      "13502\n",
      "13513\n",
      "13515\n",
      "13523\n",
      "13532\n",
      "13541\n",
      "13590\n",
      "13648\n",
      "13657\n",
      "13725\n",
      "13730\n",
      "13732\n",
      "13741\n",
      "13749\n",
      "13798\n",
      "13815\n",
      "13822\n",
      "13846\n",
      "13856\n",
      "13904\n",
      "13918\n",
      "13938\n",
      "14002\n",
      "14012\n",
      "14034\n",
      "14041\n",
      "14049\n",
      "14063\n",
      "14104\n",
      "14133\n",
      "14164\n",
      "14211\n",
      "14218\n",
      "14252\n",
      "14279\n",
      "14280\n",
      "14294\n",
      "14310\n",
      "14317\n",
      "14320\n",
      "14378\n",
      "14424\n",
      "14433\n",
      "14459\n",
      "14485\n",
      "14490\n",
      "14497\n",
      "14513\n",
      "14515\n",
      "14526\n",
      "14528\n",
      "14541\n",
      "14631\n",
      "14686\n",
      "14735\n",
      "14763\n",
      "14776\n",
      "14891\n",
      "14959\n",
      "14983\n",
      "14997\n",
      "15058\n",
      "15092\n",
      "15121\n",
      "15132\n",
      "15150\n",
      "15185\n",
      "15201\n",
      "15203\n",
      "15279\n",
      "15293\n",
      "15397\n",
      "15418\n",
      "15457\n",
      "15459\n",
      "15494\n",
      "15523\n",
      "15534\n",
      "15555\n",
      "15556\n",
      "15559\n",
      "15581\n",
      "15588\n",
      "15620\n",
      "15673\n",
      "15693\n",
      "15724\n",
      "15725\n",
      "15745\n",
      "15812\n",
      "15820\n",
      "15821\n",
      "15834\n",
      "15843\n",
      "15863\n",
      "15872\n",
      "15890\n",
      "15904\n",
      "15909\n",
      "15917\n",
      "15949\n",
      "15964\n",
      "15981\n",
      "16001\n",
      "16002\n",
      "16017\n",
      "16019\n",
      "16035\n",
      "16048\n",
      "16090\n",
      "16094\n",
      "16123\n",
      "16194\n",
      "16222\n",
      "16264\n",
      "16287\n",
      "16299\n",
      "16313\n",
      "16316\n",
      "16327\n",
      "16372\n",
      "16377\n",
      "16392\n",
      "16431\n",
      "16460\n",
      "16472\n",
      "16476\n",
      "16491\n",
      "16514\n",
      "16549\n",
      "16575\n",
      "16576\n",
      "16606\n",
      "16638\n",
      "16663\n",
      "16688\n",
      "16708\n",
      "16749\n",
      "16764\n",
      "16809\n",
      "16831\n",
      "16888\n",
      "16895\n",
      "16915\n",
      "16918\n",
      "16991\n",
      "16997\n",
      "17148\n",
      "17214\n",
      "17215\n",
      "17218\n",
      "17220\n",
      "17251\n",
      "17289\n",
      "17295\n",
      "17296\n",
      "17393\n",
      "17461\n",
      "17463\n",
      "17490\n",
      "17508\n",
      "17525\n",
      "17529\n",
      "17563\n",
      "17577\n",
      "17600\n",
      "17609\n",
      "17620\n",
      "17703\n",
      "17728\n",
      "17778\n",
      "17836\n",
      "17837\n",
      "17886\n",
      "17909\n",
      "17911\n",
      "17932\n",
      "17946\n",
      "17979\n",
      "17984\n",
      "17987\n",
      "17994\n",
      "17995\n",
      "18121\n",
      "18131\n",
      "18205\n",
      "18244\n",
      "18245\n",
      "18252\n",
      "18253\n",
      "18263\n",
      "18267\n",
      "18398\n",
      "18403\n",
      "18443\n",
      "18455\n",
      "18476\n",
      "18492\n",
      "18564\n",
      "18593\n",
      "18626\n",
      "18681\n",
      "18683\n",
      "18712\n",
      "18720\n",
      "18769\n",
      "18846\n",
      "18848\n",
      "18905\n",
      "18916\n",
      "18928\n",
      "18930\n",
      "18941\n",
      "18944\n",
      "19028\n",
      "19048\n",
      "19060\n",
      "19065\n",
      "19072\n",
      "19100\n",
      "19127\n",
      "19134\n",
      "19149\n",
      "19191\n",
      "19210\n",
      "19226\n",
      "19229\n",
      "19234\n",
      "19277\n",
      "19311\n",
      "19317\n",
      "19339\n",
      "19346\n",
      "19367\n",
      "19419\n",
      "19474\n",
      "19481\n",
      "19483\n",
      "19520\n",
      "19587\n",
      "19616\n",
      "19631\n",
      "19643\n",
      "19660\n",
      "19683\n",
      "19696\n",
      "19705\n",
      "19765\n",
      "19785\n",
      "19815\n",
      "19819\n",
      "19871\n",
      "19888\n",
      "19923\n",
      "19925\n",
      "19926\n",
      "19946\n",
      "19955\n",
      "19961\n",
      "19980\n",
      "20004\n",
      "20014\n",
      "20023\n",
      "20032\n",
      "20049\n",
      "20051\n",
      "20064\n",
      "20066\n",
      "20077\n",
      "20110\n",
      "20158\n",
      "20162\n",
      "20163\n",
      "20179\n",
      "20186\n",
      "20199\n",
      "20226\n",
      "20251\n",
      "20257\n",
      "20287\n",
      "20289\n",
      "20307\n",
      "20331\n",
      "20447\n",
      "20460\n",
      "20474\n",
      "20477\n",
      "20508\n",
      "20529\n",
      "20556\n",
      "20557\n",
      "20564\n",
      "20565\n",
      "20676\n",
      "20696\n",
      "20713\n",
      "20796\n",
      "20800\n",
      "20829\n",
      "20845\n",
      "20854\n",
      "20907\n",
      "20911\n",
      "20946\n",
      "20948\n",
      "20955\n",
      "20976\n",
      "21029\n",
      "21035\n",
      "21052\n",
      "21054\n",
      "21061\n",
      "21117\n",
      "21128\n",
      "21133\n",
      "21151\n",
      "21218\n",
      "21220\n",
      "21233\n",
      "21242\n",
      "21279\n",
      "21283\n",
      "21288\n",
      "21296\n",
      "21299\n",
      "21320\n",
      "21330\n",
      "21344\n",
      "21409\n",
      "21411\n",
      "21500\n",
      "21501\n",
      "21524\n",
      "21561\n",
      "21567\n",
      "21589\n",
      "21593\n",
      "21628\n",
      "21649\n",
      "21670\n",
      "21786\n",
      "21794\n",
      "21828\n",
      "21905\n",
      "21939\n",
      "21952\n",
      "21959\n",
      "21974\n",
      "22010\n",
      "22043\n",
      "22078\n",
      "22096\n",
      "22101\n",
      "22178\n",
      "22179\n",
      "22207\n",
      "22220\n",
      "22224\n",
      "22226\n",
      "22227\n",
      "22265\n",
      "22304\n",
      "22312\n",
      "22320\n",
      "22354\n",
      "22406\n",
      "22446\n",
      "22448\n",
      "22476\n",
      "22516\n",
      "22573\n",
      "22590\n",
      "22607\n",
      "22631\n",
      "22659\n",
      "22692\n",
      "22714\n",
      "22719\n",
      "22732\n",
      "22748\n",
      "22769\n",
      "22783\n",
      "22799\n",
      "22800\n",
      "22831\n",
      "22842\n",
      "22884\n",
      "22887\n",
      "22904\n",
      "22908\n",
      "22911\n",
      "22926\n",
      "22945\n",
      "22956\n",
      "22975\n",
      "23075\n",
      "23082\n",
      "23119\n",
      "23191\n",
      "23196\n",
      "23228\n",
      "23243\n",
      "23255\n",
      "23265\n",
      "23291\n",
      "23303\n",
      "23309\n",
      "23321\n",
      "23351\n",
      "23365\n",
      "23412\n",
      "23443\n",
      "23445\n",
      "23472\n",
      "23480\n",
      "23484\n",
      "23495\n",
      "23507\n",
      "23517\n",
      "23545\n",
      "23574\n",
      "23594\n",
      "23618\n",
      "23630\n",
      "23654\n",
      "23680\n",
      "23685\n",
      "23719\n",
      "23736\n",
      "23742\n",
      "23751\n",
      "23769\n",
      "23812\n",
      "23870\n",
      "23884\n",
      "23915\n",
      "23916\n",
      "23923\n",
      "23947\n",
      "23951\n",
      "23966\n",
      "23974\n",
      "24001\n",
      "24016\n",
      "24057\n",
      "24112\n",
      "24140\n",
      "24173\n",
      "24192\n",
      "24223\n",
      "24232\n",
      "24324\n",
      "24335\n",
      "24337\n",
      "24410\n",
      "24447\n",
      "24469\n",
      "24477\n",
      "24478\n",
      "24481\n",
      "24489\n",
      "24497\n",
      "24501\n",
      "24507\n",
      "24555\n",
      "24558\n",
      "24559\n",
      "24573\n",
      "24610\n",
      "24623\n",
      "24650\n",
      "24691\n",
      "24743\n",
      "24792\n",
      "24795\n",
      "24797\n",
      "24802\n",
      "24833\n",
      "24837\n",
      "24844\n",
      "24910\n",
      "24917\n",
      "24949\n",
      "24957\n",
      "25001\n",
      "25049\n",
      "25083\n",
      "25084\n",
      "25085\n",
      "25104\n",
      "25135\n",
      "25150\n",
      "25160\n",
      "25196\n",
      "25221\n",
      "25224\n",
      "25243\n",
      "25252\n",
      "25257\n",
      "25274\n",
      "25291\n",
      "25294\n",
      "25341\n",
      "25349\n",
      "25374\n",
      "25384\n",
      "25388\n",
      "25416\n",
      "25430\n",
      "25457\n",
      "25473\n",
      "25493\n",
      "25509\n",
      "25519\n",
      "25543\n",
      "25595\n",
      "25597\n",
      "25609\n",
      "25620\n",
      "25622\n",
      "25640\n",
      "25655\n",
      "25706\n",
      "25708\n",
      "25723\n",
      "25771\n",
      "25779\n",
      "25815\n",
      "25895\n",
      "25896\n",
      "25905\n",
      "25916\n",
      "25950\n",
      "25961\n",
      "25999\n",
      "26001\n",
      "26021\n",
      "26023\n",
      "26060\n",
      "26074\n",
      "26077\n",
      "26119\n",
      "26241\n",
      "26371\n",
      "26407\n",
      "26431\n",
      "26457\n",
      "26474\n",
      "26490\n",
      "26497\n",
      "26517\n",
      "26640\n",
      "26688\n",
      "26693\n",
      "26705\n",
      "26711\n",
      "26748\n",
      "26811\n",
      "26837\n",
      "26857\n",
      "26887\n",
      "26936\n",
      "26939\n",
      "26952\n",
      "26957\n",
      "26971\n",
      "26979\n",
      "26995\n",
      "26996\n",
      "27016\n",
      "27078\n",
      "27099\n",
      "27144\n",
      "27148\n",
      "27195\n",
      "27206\n",
      "27208\n",
      "27229\n",
      "27232\n",
      "27284\n",
      "27290\n",
      "27339\n",
      "27362\n",
      "27387\n",
      "27494\n",
      "27501\n",
      "27549\n",
      "27569\n",
      "27579\n",
      "27580\n",
      "27589\n",
      "27635\n",
      "27646\n",
      "27664\n",
      "27701\n",
      "27717\n",
      "27725\n",
      "27729\n",
      "27754\n",
      "27763\n",
      "27765\n",
      "27837\n",
      "27845\n",
      "27866\n",
      "27884\n",
      "27896\n",
      "27902\n",
      "27904\n",
      "27936\n",
      "27951\n",
      "27964\n",
      "27987\n",
      "28009\n",
      "28029\n",
      "28052\n",
      "28090\n",
      "28119\n",
      "28152\n",
      "28174\n",
      "28253\n",
      "28268\n",
      "28293\n",
      "28312\n",
      "28381\n",
      "28471\n",
      "28473\n",
      "28486\n",
      "28566\n",
      "28571\n",
      "28586\n",
      "28610\n",
      "28616\n",
      "28644\n",
      "28649\n",
      "28704\n",
      "28738\n",
      "28754\n",
      "28761\n",
      "28818\n",
      "28823\n",
      "28829\n",
      "28858\n",
      "28951\n",
      "28967\n",
      "28983\n",
      "29029\n",
      "29032\n",
      "29072\n",
      "29076\n",
      "29117\n",
      "29134\n",
      "29210\n",
      "29302\n",
      "29316\n",
      "29341\n",
      "29345\n",
      "29390\n",
      "29438\n",
      "29456\n",
      "29486\n",
      "29498\n",
      "29507\n",
      "29537\n",
      "29621\n",
      "29634\n",
      "29655\n",
      "29669\n",
      "29712\n",
      "29745\n",
      "29783\n",
      "29789\n",
      "29808\n",
      "29813\n",
      "29826\n",
      "29854\n",
      "29855\n",
      "29861\n",
      "29908\n",
      "29950\n",
      "29954\n",
      "29965\n",
      "29989\n",
      "30002\n",
      "30047\n",
      "30068\n",
      "30073\n",
      "30093\n",
      "30099\n",
      "30120\n",
      "30145\n",
      "30162\n",
      "30211\n",
      "30238\n",
      "30247\n",
      "30261\n",
      "30296\n",
      "30309\n",
      "30346\n",
      "30440\n",
      "30518\n",
      "30570\n",
      "30574\n",
      "30623\n",
      "30634\n",
      "30670\n",
      "30736\n",
      "30739\n",
      "30740\n",
      "30817\n",
      "30825\n",
      "30854\n",
      "30867\n",
      "30876\n",
      "30883\n",
      "30890\n",
      "30893\n",
      "30904\n",
      "30942\n",
      "30944\n",
      "30975\n",
      "31018\n",
      "24.096385542168676\n",
      "42.42424242424242\n",
      "43.75\n",
      "100.0\n",
      "--- 30.252775192260742 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from nltk.stem import PorterStemmer\n",
    "import time\n",
    "import string\n",
    "start_time = time.time()\n",
    "\n",
    "### CREATION STOP WORDS !!!   \n",
    "with open('stopwords/de') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_de = []\n",
    "for i in mylist:\n",
    "    stopwords_de.append(i)\n",
    "    \n",
    "with open('stopwords/en') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_en = []\n",
    "for i in mylist:\n",
    "    stopwords_en.append(i)\n",
    "    \n",
    "with open('stopwords/it') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_it = []\n",
    "for i in mylist:\n",
    "    stopwords_it.append(i)\n",
    "    \n",
    "with open('stopwords/fr') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_fr = []\n",
    "for i in mylist:\n",
    "    stopwords_fr.append(i)\n",
    "### FIN CREATION STOPWORDS\n",
    "    \n",
    "all_filtered_sentence_en=[]\n",
    "all_filtered_sentence_de=[]\n",
    "all_filtered_sentence_fr=[]\n",
    "all_filtered_sentence_it=[]\n",
    "    \n",
    "def filtering(stop_words, index):\n",
    "    sentence=twitterWith.iloc[index][\"_source\"].get('main');\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    words_filtered=[x.lower() for x in word_tokens if x not in stop_words if x not in string.punctuation if len(x)>=3 ]\n",
    "    return words_filtered\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in get_word_features(document):\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "twitterWith=twitter\n",
    "for i in range(0,len(twitter)):\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        print(i)\n",
    "        twitterWith.drop(twitterWith.index[[i]])\n",
    "        \n",
    "twitterWith = twitterWith.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "twitter['language'] = pd.Series(np.zeros(len(twitter)));\n",
    "\n",
    "for i in range(0,1000): #TO CHANGE !!! len(twitterWith)\n",
    "\n",
    "    #ADD COLUMN TO ADD THE LANGUAGE\n",
    "    sentence=twitterWith.iloc[i][\"_source\"].get('main')\n",
    "    language='None';\n",
    "    try:\n",
    "        language=detect(sentence)\n",
    "    except:\n",
    "        e=1;\n",
    "        \n",
    "    twitterWith.ix[i,'language']=language\n",
    "\n",
    "    if language=='en':\n",
    "        words_filtered=filtering(stopwords_en,i)\n",
    "        all_filtered_sentence_en.append((words_filtered,twitterWith._source[i].get('sentiment'))) \n",
    "    elif language=='fr':\n",
    "        words_filtered=filtering(stopwords_fr,i)\n",
    "        all_filtered_sentence_fr.append((words_filtered,twitterWith._source[i].get('sentiment')))\n",
    "    elif language=='de':\n",
    "        words_filtered=filtering(stopwords_de,i)\n",
    "        all_filtered_sentence_de.append((words_filtered,twitterWith._source[i].get('sentiment')))\n",
    "    elif language=='it':\n",
    "        words_filtered=filtering(stopwords_it,i)\n",
    "        all_filtered_sentence_it.append((words_filtered,twitterWith._source[i].get('sentiment')))                        \n",
    "\n",
    "\n",
    "#CLASSIFYING\n",
    "#word_features_en = get_word_features(get_words_in_tweets(all_filtered_sentence_en))\n",
    "all_filtered_sentence_en_train=all_filtered_sentence_en[:round(0.8*len(all_filtered_sentence_en))]\n",
    "#for i in range(0,len(all_filtered_sentence_en_train)):\n",
    "#    training_set_en.append(extract_features(all_filtered_sentence_en_train[i],word_features_en))\n",
    "training_set_en = nltk.classify.apply_features(extract_features, all_filtered_sentence_en_train)\n",
    "classifier_en = nltk.NaiveBayesClassifier.train(training_set_en)\n",
    "\n",
    "#word_features_fr = get_word_features(get_words_in_tweets(all_filtered_sentence_fr))\n",
    "all_filtered_sentence_fr_train=all_filtered_sentence_fr[:round(0.8*len(all_filtered_sentence_fr))]\n",
    "training_set_fr = nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_train)\n",
    "classifier_fr = nltk.NaiveBayesClassifier.train(training_set_fr)\n",
    "\n",
    "#word_features_de = get_word_features(get_words_in_tweets(all_filtered_sentence_de))\n",
    "all_filtered_sentence_de_train=all_filtered_sentence_de[:round(0.8*len(all_filtered_sentence_de))]\n",
    "training_set_de = nltk.classify.apply_features(extract_features, all_filtered_sentence_de_train)\n",
    "classifier_de = nltk.NaiveBayesClassifier.train(training_set_de)\n",
    "\n",
    "#word_features_it = get_word_features(get_words_in_tweets(all_filtered_sentence_it))\n",
    "all_filtered_sentence_it_train=all_filtered_sentence_it[:round(0.8*len(all_filtered_sentence_it))]\n",
    "training_set_it = nltk.classify.apply_features(extract_features, all_filtered_sentence_it_train)\n",
    "classifier_it = nltk.NaiveBayesClassifier.train(training_set_it)\n",
    "\n",
    "\n",
    "#PRINT RESULTS\n",
    "all_filtered_sentence_en_test=all_filtered_sentence_en[round(0.8*len(all_filtered_sentence_en)):]\n",
    "testing_set_en = nltk.classify.apply_features(extract_features, all_filtered_sentence_en_test)\n",
    "print(nltk.classify.accuracy(classifier_en, testing_set_en)*100)\n",
    "\n",
    "all_filtered_sentence_fr_test=all_filtered_sentence_fr[round(0.8*len(all_filtered_sentence_fr)):]\n",
    "testing_set_fr = nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_test)\n",
    "print(nltk.classify.accuracy(classifier_fr, testing_set_fr)*100)\n",
    "\n",
    "all_filtered_sentence_de_test=all_filtered_sentence_de[round(0.8*len(all_filtered_sentence_de)):]\n",
    "testing_set_de = nltk.classify.apply_features(extract_features, all_filtered_sentence_de_test)\n",
    "print(nltk.classify.accuracy(classifier_de, testing_set_de)*100)\n",
    "\n",
    "all_filtered_sentence_it_test=all_filtered_sentence_it[round(0.8*len(all_filtered_sentence_it)):]\n",
    "testing_set_it = nltk.classify.apply_features(extract_features, all_filtered_sentence_it_test)\n",
    "print(nltk.classify.accuracy(classifier_it, testing_set_it)*100)\n",
    "\n",
    "####\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31058\n",
      "31058\n"
     ]
    }
   ],
   "source": [
    "print(len(twitter))\n",
    "print(len(twitterWith))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 29783 is out of bounds for axis 1 with size 29771",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-98f45cd18b1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_source\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtwitterWith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtwitterWith\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitterWith\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/cyrilschmitt/anaconda/lib/python3.5/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpromote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 29783 is out of bounds for axis 1 with size 29771"
     ]
    }
   ],
   "source": [
    "twitterWith=twitter\n",
    "for i in range(0,len(twitter)):\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        twitterWith.drop(twitterWith.index[[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "POSITIVE\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "None\n",
      "POSITIVE\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "POSITIVE\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "POSITIVE\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "None\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEGATIVE\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "None\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "POSITIVE\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n",
      "NEUTRAL\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,400):\n",
    "    print(twitterWith._source[i].get('sentiment'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
