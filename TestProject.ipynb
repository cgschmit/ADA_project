{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give the average sentiment PER CANTON or per city (more precise is imposible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "news=pd.read_json(\"harvest3r_news_data_13-01_0.json\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2315"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trotz des Streits unter den Kantonen um die begehrten Bundesgelder hat sich eine Mehrheit für die zweite Röhre ausgesprochen. Sogar der Kanton Zürich, dessen Städte täglich ein viel höheres Verkehrsaufkommen (100 000 Autos) bewältigen müssen als der Gotthard an verkehrsreichen Tagen (17 000 Autos), befürwortet die Sanierung samt zweiter Röhre. Der Zürcher SVP-Regierungsrat Markus Kägi verlangte an einer Medienkonferenz im Herbst mehr Solidarität. Jeder Kanton sei für Grossprojekte auf die Gunst der anderen angewiesen, so Kägi und verwies auf anstehende Zürcher Projekte, die der Bund finanzieren wird: Der Gubrist für 1,5 Milliarden Franken ist baureif. Die Umfahrung Winterthur samt Glattalautobahn für 3,7 Milliarden Franken gehört zur Planung des Bundesrats. Kägi: «Die Kosten für den Gotthard sind zwar hoch, aber nicht aussergewöhnlich.» Und: Genauso wie Zürich auf die Solidarität der Kantone zählen könne, sei auch das Tessin darauf angewiesen.\n",
      "\n",
      "\n",
      "Liestals Stadtpräsident Lukas Ott, der zusammen mit Tobias Eggimann, Geschäftsführer von Baselland Tourismus, und Lukas Kilcher, Leiter des Landwirtschaftlichen Zentrums Ebenrain, das Organisationskomitee dieses besonderen Genussjahres bildet, meinte am Dienstag zum Startschuss: «Wir sind nicht alleine, sondern haben mit den Bauern, Produzenten, Verarbeitern, Metzgern, Winzern und Gastronomen starke Partner. Sie machen unser Potenzial aus, sie füllen die Schatztruhe mit ihren Produkten. Das hat auch das nationale Komitee überzeugt.»\n",
      "\n",
      "Was heisst das nun konkret? Tobias Eggimann kündete ein Programm an, das bis im Herbst monatlich mindestens einen Genuss-Anlass bietet, wobei teilweise auch bereits bestehende Veranstaltungen ausgebaut werden. So kann man zum Beispiel an den beiden letzten Freitagabenden im Januar in sieben Restaurants auf der Wasserfallen Fondue essen (und Gondelbahn fahren), am Chienbäse-Umzug gibt es auf dem Liestaler Postplatz diverse Verpflegungsstände mit regionalen Produkten, von März bis Juni wird der Liestaler Bauernmarkt probeweise erweitert, um das Potenzial auf der Angebots- und Nachfrageseite für die Zukunft zu eruieren, und im August wird das traditionelle und jeweils innerhalb von Minuten ausverkaufte «Wy-Erläbnis» ausgebaut.\n",
      "\n",
      "\n",
      "In Afrika ist die traditionelle Musik so allgegenwärtig, dass man sie weder pflegen noch inszenieren muss. Natürlich gibt es auch dort die Sänger, die klagen, dass die Jungen nur noch Hip-Hop und Elektrozeugs hören. Das 20.?Jahrhundert ist voller europäischer Ethnologen, die durch Afrika und Arabien reisten, um «das Erbe zu bewahren». Viele afrikanische Musiker haben kein Problem damit, beides zu machen. Sekouba Bambino etwa kommt aus einer Griot-Familie mit tausendjähriger Musiktradition, die selbstverständlich auch er beherrscht. Daneben macht er Elektromusik für die Tanzpartys. Der Umgang mit dem ­kulturellen Erbe ist spielerischer geworden; man verwendet auch bedenkenlos Sounds aus der Medien- und Popkultur. Als sich Weltmusiker noch stärker am westlichen Markt orientieren mussten, ging es darum, eine vermeintlich tra­ditionelle Musik zu nehmen, sie auf ­Ho­chglanz zu polieren und zu den ­europäischen Festivals zu tragen.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3,6):\n",
    "    print(news._source[i].get('summary_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bucket': 1452710100052,\n",
       " 'canonical': 'http://www.basellandschaftlichezeitung.ch/basel/baselbiet/liestal-ist-nationale-genussstadt-und-hat-damit-auch-neue-pflichten-129985163',\n",
       " 'card': 'SUMMARY_LARGE_IMAGE',\n",
       " 'date_found': '2016-01-13T18:36:15Z',\n",
       " 'description': 'Liestal ist für ein Jahr nationaler Mittelpunkt einer regionalen, frischen Küche. Um der Auszeichnung als Schweizer Genussstadt des Jahres 2016 gerecht zu werden, wird das Stedtli monatlich mindestens einen Genuss-Anlass veranstalten.',\n",
       " 'detection_method': 'FEED',\n",
       " 'domain': 'basellandschaftlichezeitung.ch',\n",
       " 'duplicates_count': 0,\n",
       " 'extract': '<a href=\"http://www.basellandschaftlichezeitung.ch/basel/baselbiet/liestal-ist-nationale-genussstadt-und-hat-damit-auch-neue-pflichten-129985163#comment-jumpto\"> Kommentare </a><p>Locarno, Moutier, Lausanne, Bellinzona, Luzern – so hiessen die Schweizer Genussst&auml;dte der letzten Jahre. F&uuml;r 2016 hat nun das nationale Komitee, das jeweils f&uuml;r die herbstliche Genusswoche verantwortlich zeichnet, Liestal auf den Genussstadt-Schild gehoben. Ein Titel, der f&uuml;r Liestal nicht nur W&uuml;rde, sondern auch B&uuml;rde ist. Denn die Genussstadt des Jahres nimmt nicht nur an der gesamtschweizerischen Genusswoche im September teil, wie es Liestal und das Baselbiet seit drei Jahren machen, sondern von ihr wird ein besonderer Effort in Sachen F&ouml;rderung der regionalen, saisonalen und frischen K&uuml;che erwartet.</p><p>Liestals Stadtpr&auml;sident Lukas Ott, der zusammen mit Tobias Eggimann, Gesch&auml;ftsf&uuml;hrer von Baselland Tourismus, und Lukas Kilcher, Leiter des Landwirtschaftlichen Zentrums Ebenrain, das Organisationskomitee dieses besonderen Genussjahres bildet, meinte am Dienstag&nbsp;zum Startschuss: &laquo;Wir sind nicht alleine, sondern haben mit den Bauern, Produzenten, Verarbeitern, Metzgern, Winzern und Gastronomen starke Partner. Sie machen unser Potenzial aus, sie f&uuml;llen die Schatztruhe mit ihren Produkten. Das hat auch das nationale Komitee &uuml;berzeugt.&raquo;</p><p>Was heisst das nun konkret? Tobias Eggimann k&uuml;ndete ein Programm an, das bis im Herbst monatlich mindestens einen Genuss-Anlass bietet, wobei teilweise auch bereits bestehende Veranstaltungen ausgebaut werden. So kann man zum Beispiel an den beiden letzten Freitagabenden im Januar in sieben Restaurants auf der Wasserfallen Fondue essen (und Gondelbahn fahren), am Chienb&auml;se-Umzug gibt es auf dem Liestaler Postplatz diverse Verpflegungsst&auml;nde mit regionalen Produkten, von M&auml;rz bis Juni wird der Liestaler Bauernmarkt probeweise erweitert, um das Potenzial auf der Angebots- und Nachfrageseite f&uuml;r die Zukunft zu eruieren, und im August wird das traditionelle und jeweils innerhalb von Minuten ausverkaufte &laquo;Wy-Erl&auml;bnis&raquo; ausgebaut.</p><p>Das Hauptprogramm folgt dann im September mit dem &laquo;Abschluss-Event&raquo; am 25.9.: Die Liestaler Rathausstrasse wird vom T&ouml;rli bis zum Regierungsgeb&auml;ude zu einer langen Tafel f&uuml;r 700 Leute, an der Baselbieter Speisen serviert werden und &laquo;Viva Cello&raquo; f&uuml;r die musikalische Begleitung sorgt. Ott sprach von einem &laquo;Volksfest&raquo;, das wie das ganze Angebot im Rahmen der Genussstadt Liestal &laquo;etwas Geerdetes und nichts Abgehobenes&raquo; sein soll.</p><h2>Gastgeber-Rolle lebt wieder auf</h2><p>Dass gerade Liestal Genussstadt wird, begr&uuml;ndet Ott auch mit dessen Geschichte: Liestals Gastgeber-Rolle gehe auf die Zeit zur&uuml;ck, als der Nord-S&uuml;d-Transitverkehr zunahm. Liestal sei damals wichtiger Etappenort und H&uuml;ter der beiden Hauenstein-&Uuml;berg&auml;nge gewesen und habe mehr als die eigene Bev&ouml;lkerung unterbringen und verpflegen m&uuml;ssen. Davon zeugten die einst zahlreichen B&auml;ckereien, Metzgereien und Gasth&ouml;fe. &laquo;Wir wollen diese Rolle jetzt wieder betonen&raquo;, sagte Ott und f&uuml;gte bei: &laquo;Ich bin stolz, in diesem Jahr Stadtpr&auml;sident der Schweizer Genussstadt zu sein.&raquo;</p><p>Auch der Ebenrain will seinen Beitrag zum Baselbieter &laquo;Genuss-Ausnahmezustand&raquo; leisten, wie Lukas Kilcher ank&uuml;ndete. Dazu geh&ouml;rt die Erarbeitung eines Buchs zu regionalen Produkten und Rezepten, das im Herbst pr&auml;sentiert werde. Kilcher verwies darauf, dass das Baselbiet kulinarisch mit der restlichen Schweiz mehr als bloss mithalten k&ouml;nne und am letztj&auml;hrigen Wettbewerb der Regionalprodukte in Del&eacute;mont 13 Medaillen abholte. Der Ebenrain will k&uuml;nftig Baselbieter Produkte, die Gastronomie und M&auml;rkte, die die Kriterien bez&uuml;glich Herkunft und Qualit&auml;t erf&uuml;llen, mit einer Dachmarke auszeichnen.</p><p>Am Informationsanlass am Dienstag im Liestaler Rathaus gab es aber auch einen D&auml;mpfer: Zum abschliessenden Ap&eacute;ro war die Regierung in corpore angek&uuml;ndigt. Doch diese erachtete den Anlass, so war unter der Hand zu h&ouml;ren, als zu wenig wichtig, um teilzunehmen. Offiziell hat eine Terminkollision ihre Pr&auml;senz verhindert.</p><div>\\n  (bz) \\n</div>',\n",
       " 'extract_checksum': 'tWotNDf3EhJgmjAvUa3aX2WrNYw',\n",
       " 'extract_length': 4448,\n",
       " 'hashcode': '2kooewXIoaRDdjUIRJ2REzGt4vA',\n",
       " 'image_src': 'http://static.a-z.ch/__ip/vfxiiDm5ONrD0RVp2C23WNKl0fE/f6cac649ad8165958baeb600407f77d068a040e5/teaser-medium/lukas-kilcher-lukas-ott-und-tobias-eggimann-von-links-stossen-mit-liestaler-wein-auf-liestal-als-schweizer-genussstadt-des-jahres-2016-an-',\n",
       " 'index_method': 'PERMALINK_TASK',\n",
       " 'lang': 'de',\n",
       " 'permalink': 'http://www.basellandschaftlichezeitung.ch/basel/baselbiet/liestal-ist-nationale-genussstadt-und-hat-damit-auch-neue-pflichten-129985163',\n",
       " 'permalink_redirect': 'http://www.basellandschaftlichezeitung.ch/basel/baselbiet/liestal-ist-nationale-genussstadt-und-hat-damit-auch-neue-pflichten-129985163',\n",
       " 'permalink_redirect_domain': 'basellandschaftlichezeitung.ch',\n",
       " 'permalink_redirect_site': 'basellandschaftlichezeitung.ch',\n",
       " 'published': '2016-01-13T17:49:21Z',\n",
       " 'publisher': 'Basellandschaftliche Zeitung',\n",
       " 'resource': 'http://basellandschaftlichezeitung.ch/basel/baselbiet/liestal-ist-nationale-genussstadt-und-hat-damit-auch-neue-pflichten-129985163',\n",
       " 'sequence': 1452710175000004465,\n",
       " 'sequence_range': 84752,\n",
       " 'site': 'basellandschaftlichezeitung.ch',\n",
       " 'source_date_found': '2010-04-06T05:00:00Z',\n",
       " 'source_hashcode': '_IQvO0c8wYC8gmw95HDWg2OWRno',\n",
       " 'source_link': 'http://www.bz-online.ch',\n",
       " 'source_publisher_type': 'WEBLOG',\n",
       " 'source_resource': 'http://bz-online.ch',\n",
       " 'source_setting_update_strategy': 'CYCLICAL',\n",
       " 'source_update_interval': 3600000,\n",
       " 'summary_text': 'Liestals Stadtpräsident Lukas Ott, der zusammen mit Tobias Eggimann, Geschäftsführer von Baselland Tourismus, und Lukas Kilcher, Leiter des Landwirtschaftlichen Zentrums Ebenrain, das Organisationskomitee dieses besonderen Genussjahres bildet, meinte am Dienstag\\xa0zum Startschuss: «Wir sind nicht alleine, sondern haben mit den Bauern, Produzenten, Verarbeitern, Metzgern, Winzern und Gastronomen starke Partner. Sie machen unser Potenzial aus, sie füllen die Schatztruhe mit ihren Produkten. Das hat auch das nationale Komitee überzeugt.»\\n\\nWas heisst das nun konkret? Tobias Eggimann kündete ein Programm an, das bis im Herbst monatlich mindestens einen Genuss-Anlass bietet, wobei teilweise auch bereits bestehende Veranstaltungen ausgebaut werden. So kann man zum Beispiel an den beiden letzten Freitagabenden im Januar in sieben Restaurants auf der Wasserfallen Fondue essen (und Gondelbahn fahren), am Chienbäse-Umzug gibt es auf dem Liestaler Postplatz diverse Verpflegungsstände mit regionalen Produkten, von März bis Juni wird der Liestaler Bauernmarkt probeweise erweitert, um das Potenzial auf der Angebots- und Nachfrageseite für die Zukunft zu eruieren, und im August wird das traditionelle und jeweils innerhalb von Minuten ausverkaufte «Wy-Erläbnis» ausgebaut.\\n\\n',\n",
       " 'title': 'Liestal ist nationale Genussstadt - und hat damit auch neue Pflichten',\n",
       " 'type': 'POST',\n",
       " 'version': '5.1.614'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news._source[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Merger (on the server)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "files=[name for name in os.listdir('.') if os.path.isfile(name)];\n",
    "AllTwitter=[];\n",
    "AllInstagram=[];\n",
    "AllNews=[];\n",
    "a=0;\n",
    "#Add Loop \"for\" to go in different directory (january to october)\n",
    "for i in ['january','february','march','april','mai','june','july','august','september','october']:\n",
    "    files=[name for name in os.listdir('./'+i+\"/\") if os.path.isfile(name)]; #Change the path accordingly\n",
    "    for i in files:\n",
    "        if files[a].find('twitter')>0:\n",
    "            try:\n",
    "                twitter=pd.read_json(i);\n",
    "                AllTwitter.append(twitter);\n",
    "            except ValueError:\n",
    "                print (\"JSON object issue\");\n",
    "        elif files[a].find('instagram')>0:\n",
    "            try:\n",
    "                instagram=pd.read_json(i);\n",
    "                AllInstagram.append(instagram);\n",
    "            except ValueError:\n",
    "                print (\"JSON object issue\");\n",
    "        elif files[a].find('news')>0:\n",
    "            try:\n",
    "                news=pd.read_json(i);\n",
    "                AllNews.append(news);\n",
    "            except ValueError:\n",
    "                print (\"JSON object issue\");\n",
    "        a=a+1;\n",
    "\n",
    "#To transform into DataFrame\n",
    "AllTwitter=pd.concat(AllTwitter, ignore_index=True); #To transform into DataFrame\n",
    "AllInstagram=pd.concat(AllInstagram, ignore_index=True);\n",
    "AllNews=pd.concat(AllNews, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File merger (locally)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON object issue\n"
     ]
    }
   ],
   "source": [
    "import os, os.path\n",
    "files=[name for name in os.listdir('.') if os.path.isfile(name)];\n",
    "AllTwitter=[];\n",
    "AllInstagram=[];\n",
    "AllNews=[];\n",
    "a=0;\n",
    "\n",
    "files=[name for name in os.listdir('.') if os.path.isfile(name)]; \n",
    "for i in files:\n",
    "    if files[a].find('twitter')>0:\n",
    "        try:\n",
    "            twitter=pd.read_json(i);\n",
    "            AllTwitter.append(twitter);\n",
    "        except ValueError:\n",
    "            print (\"JSON object issue\");\n",
    "    elif files[a].find('instagram')>0:\n",
    "        try:\n",
    "            instagram=pd.read_json(i);\n",
    "            AllInstagram.append(instagram);\n",
    "        except ValueError:\n",
    "            print (\"JSON object issue\");\n",
    "    elif files[a].find('news')>0:\n",
    "        try:\n",
    "            news=pd.read_json(i);\n",
    "            AllNews.append(news);\n",
    "        except ValueError:\n",
    "            print (\"JSON object issue\");\n",
    "    a=a+1;\n",
    "\n",
    "#To transform into DataFrame\n",
    "AllTwitter=pd.concat(AllTwitter, ignore_index=True); #To transform into DataFrame\n",
    "AllInstagram=pd.concat(AllInstagram, ignore_index=True);\n",
    "AllNews=pd.concat(AllNews, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to separate row with sentiment to row without + Write in positive.txt and negative.txt + delete stop_words (useless ones)\n",
    "\n",
    "\n",
    "# I cannot concatenate, don't know why !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def deleteContent(pfile):\n",
    "    pfile.seek(0)\n",
    "    pfile.truncate()\n",
    "\n",
    "additional_stopwords=['.',',','/','(',')','[',']',':','@','\\'','?','!','\\\"',';','&']\n",
    "TwitterWithPos=[]\n",
    "TwitterWithNeg=[]\n",
    "TwitterWithout=[]\n",
    "fPos = open('positive.txt', 'r+')\n",
    "fNeg = open('negative.txt', 'r+')\n",
    "fNeu = open('neutral.txt', 'r+')\n",
    "#English\n",
    "#fPosEN = open('positive.txt', 'r+')\n",
    "#fNegEN = open('negative.txt', 'r+')\n",
    "#fNeuEN = open('neutral.txt', 'r+')\n",
    "#French\n",
    "#fPosFR = open('positive.txt', 'r+')\n",
    "#fNegFR = open('negative.txt', 'r+')\n",
    "#fNeuFR = open('neutral.txt', 'r+')\n",
    "#German\n",
    "#fPosDE = open('positive.txt', 'r+')\n",
    "#fNegDE = open('negative.txt', 'r+')\n",
    "#fNeuDE = open('neutral.txt', 'r+')\n",
    "#Italian\n",
    "#fPosIT = open('positive.txt', 'r+')\n",
    "#fNegIT = open('negative.txt', 'r+')\n",
    "#fNeuIT = open('neutral.txt', 'r+')\n",
    "\n",
    "deleteContent(fPos)\n",
    "deleteContent(fNeg)\n",
    "deleteContent(fNeu)\n",
    "twitter['language'] = pd.Series(np.zeros(len(twitter)));\n",
    "\n",
    "for i in range(0,400): #TO CHANGE !!! len(twitter)\n",
    "    sentence=twitter.iloc[i][\"_source\"].get('main');\n",
    "    language='None';\n",
    "    try:\n",
    "        language=detect(sentence); \n",
    "    except:\n",
    "        e=1;\n",
    "    twitter.ix[i,'language']=language;\n",
    "    #FIND the language of the current Tweet\n",
    "    if language=='en' or language=='de' or language=='it' or language=='fr': \n",
    "        langDict = {'en': 'english', 'de': 'german', 'it':'italian', 'fr':'french'} \n",
    "        stop_words = set(stopwords.words(langDict[language])) \n",
    "        word_tokens = word_tokenize(sentence)\n",
    "        filtered_sentence = []\n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words and w not in additional_stopwords and len(w)>=3:\n",
    "                filtered_sentence.append(w);\n",
    "    #FIND Tweets without sentiment and place it into a text file per language\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        TwitterWithout.append(twitter.iloc[[i]])\n",
    "        try:\n",
    "            filtered_sentence=' '.join(filtered_sentence)\n",
    "        except ValueError:\n",
    "            print('One-word Tweet')  \n",
    "        fNeu.write(str(filtered_sentence)+\"\\n\");\n",
    "    #FIND Tweets with sentiment and place it into a text file per language\n",
    "    else:\n",
    "        #FIND Tweet with POSITIVE sentiment\n",
    "        if twitter._source[i].get('sentiment')=='POSITIVE':\n",
    "            TwitterWithPos.append(twitter.iloc[[i]])\n",
    "            try:\n",
    "                filtered_sentence=' '.join(filtered_sentence)\n",
    "            except ValueError:\n",
    "                print('One-word Tweet') \n",
    "            fPos.write(str(filtered_sentence)+\"\\n\");\n",
    "        #FIND Tweet with NEGATIVE sentiment\n",
    "        elif twitter._source[i].get('sentiment')=='NEGATIVE':\n",
    "            TwitterWithNeg.append(twitter.iloc[[i]])\n",
    "            try:\n",
    "                filtered_sentence=' '.join(filtered_sentence)\n",
    "            except ValueError:\n",
    "                print('One-word Tweet') \n",
    "            fNeg.write(str(filtered_sentence)+\"\\n\");\n",
    "\n",
    "#To transform into DataFrame       \n",
    "TwitterWithPos=pd.concat(TwitterWithPos, ignore_index=True)\n",
    "TwitterWithNeg=pd.concat(TwitterWithNeg, ignore_index=True)\n",
    "TwitterWithout=pd.concat(TwitterWithout, ignore_index=True)\n",
    "\n",
    "fPos.close()\n",
    "fNeg.close()\n",
    "fNeu.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Last work to classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last problem still occuring:\n",
    "\n",
    "1) I have no clue how to get all the word features in \"extract_features\" function. Problem linked to the fact we're using: nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_train) \n",
    "    \n",
    "    nltk.classify.apply_features(function, toks)\n",
    "    \n",
    "    [function(tok) for tok in toks]\n",
    "    \n",
    "2) Problem to compute the accuracy. It worked at some point but could'nt fix it back with all the changes I added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming Words would be nice feat to add, but need to get one for each language..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A ELE probability distribution must have at least one bin.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-5717c43673cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;31m#    training_set_en.append(extract_features(all_filtered_sentence_en_train[i],word_features_en))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0mtraining_set_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_filtered_sentence_en_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m \u001b[0mclassifier_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;31m#word_features_fr = get_word_features(get_words_in_tweets(all_filtered_sentence_fr))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cyrilschmitt/anaconda/lib/python3.5/site-packages/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# Create the P(label) distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mlabel_probdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_freqdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Create the P(fval|label, fname) distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cyrilschmitt/anaconda/lib/python3.5/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, freqdist, bins)\u001b[0m\n\u001b[1;32m    848\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspecified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfreqdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \"\"\"\n\u001b[0;32m--> 850\u001b[0;31m         \u001b[0mLidstoneProbDist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cyrilschmitt/anaconda/lib/python3.5/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, freqdist, gamma, bins)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             raise ValueError('A %s probability distribution ' % name +\n\u001b[0;32m--> 730\u001b[0;31m                              'must have at least one bin.')\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A ELE probability distribution must have at least one bin."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from nltk.stem import PorterStemmer\n",
    "import time\n",
    "import string\n",
    "start_time = time.time()\n",
    "\n",
    "### CREATION STOP WORDS !!!   \n",
    "with open('stopwords/de') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_de = []\n",
    "for i in mylist:\n",
    "    stopwords_de.append(i)\n",
    "    \n",
    "with open('stopwords/en') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_en = []\n",
    "for i in mylist:\n",
    "    stopwords_en.append(i)\n",
    "    \n",
    "with open('stopwords/it') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_it = []\n",
    "for i in mylist:\n",
    "    stopwords_it.append(i)\n",
    "    \n",
    "with open('stopwords/fr') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_fr = []\n",
    "for i in mylist:\n",
    "    stopwords_fr.append(i)\n",
    "### FIN CREATION STOPWORDS\n",
    "    \n",
    "def filtering(stop_words, index):\n",
    "    sentence=twitterAllWith.iloc[index][\"_source\"].get('main');\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    words_filtered=[x.lower() for x in word_tokens if x not in stop_words if x not in string.punctuation if len(x)>=3 ]\n",
    "    #COULD BE ADDED IF NEEDED : if x.find('youtu.be')==-1 if x.find('twitter.com')==-1 if x.find('www')==-1 if x.find('.com')==-1\n",
    "    return words_filtered\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in get_word_features(document):\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "\"\"\"\n",
    "TwitterWithPos=[]\n",
    "TwitterWithNeg=[]\n",
    "TwitterWithNeu=[]\n",
    "TwitterWithout=[]\n",
    "twitterAllWith=[]\n",
    "all_filtered_sentence_en=[]\n",
    "all_filtered_sentence_de=[]\n",
    "all_filtered_sentence_fr=[]\n",
    "all_filtered_sentence_it=[]\n",
    "\n",
    "\n",
    "#DISSOCIATE ROW WITH/WITHOUT SENTIMENT     \n",
    "#FIND Tweets without sentiment and place it into a text file per language\n",
    "\n",
    "for i in range(0,len(twitter)):\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        TwitterWithout.append(twitter.iloc[[i]])\n",
    "    else:\n",
    "        #FIND Tweet with POSITIVE sentiment\n",
    "        if twitter._source[i].get('sentiment')=='POSITIVE':\n",
    "            TwitterWithPos.append(twitter.iloc[[i]]) \n",
    "        #FIND Tweet with NEGATIVE sentiment\n",
    "        elif twitter._source[i].get('sentiment')=='NEGATIVE':\n",
    "            TwitterWithNeg.append(twitter.iloc[[i]])\n",
    "        #FIND Tweet with NEUTRAL sentiment\n",
    "        elif twitter._source[i].get('sentiment')=='NEUTRAL':\n",
    "            TwitterWithNeu.append(twitter.iloc[[i]])\n",
    "\n",
    "#To transform into DataFrame       \n",
    "TwitterWithPos=pd.concat(TwitterWithPos, ignore_index=True)\n",
    "TwitterWithNeg=pd.concat(TwitterWithNeg, ignore_index=True)\n",
    "TwitterWithout=pd.concat(TwitterWithout, ignore_index=True)\n",
    "\n",
    "frames = [TwitterWithPos, TwitterWithNeg,TwitterWithNeu]\n",
    "twitterAllWith = pd.concat(frames)\n",
    "twitterAllWith = twitterAllWith.sample(frac=1).reset_index(drop=True) #Shuffle DF\n",
    "\"\"\"  \n",
    "TwitterWith=twitter\n",
    "for i in range(0,len(twitter)):\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        TwitterWith.drop(TwitterWith.index[[i]])\n",
    "        \n",
    "TwitterWith = TwitterWith.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "twitter['language'] = pd.Series(np.zeros(len(twitter)));\n",
    "\n",
    "for i in range(0,len(twitterAllWith)): #TO CHANGE !!! len(twitterAllWith)\n",
    "\n",
    "    #ADD COLUMN TO ADD THE LANGUAGE\n",
    "    sentence=twitterAllWith.iloc[i][\"_source\"].get('main')\n",
    "    language='None';\n",
    "    try:\n",
    "        language=detect(sentence)\n",
    "    except:\n",
    "        e=1;\n",
    "        \n",
    "    twitterAllWith.ix[i,'language']=language\n",
    "\n",
    "    if language=='en':\n",
    "        words_filtered=filtering(stopwords_en,i)\n",
    "        all_filtered_sentence_en.append((words_filtered,twitterAllWith._source[i].get('sentiment'))) \n",
    "    elif language=='fr':\n",
    "        words_filtered=filtering(stopwords_fr,i)\n",
    "        all_filtered_sentence_fr.append((words_filtered,twitterAllWith._source[i].get('sentiment')))\n",
    "    elif language=='de':\n",
    "        words_filtered=filtering(stopwords_de,i)\n",
    "        all_filtered_sentence_de.append((words_filtered,twitterAllWith._source[i].get('sentiment')))\n",
    "    elif language=='it':\n",
    "        words_filtered=filtering(stopwords_it,i)\n",
    "        all_filtered_sentence_it.append((words_filtered,twitterAllWith._source[i].get('sentiment')))                        \n",
    "\n",
    "\n",
    "#CLASSIFYING\n",
    "#word_features_en = get_word_features(get_words_in_tweets(all_filtered_sentence_en))\n",
    "all_filtered_sentence_en_train=all_filtered_sentence_en[:round(0.8*len(all_filtered_sentence_en))]\n",
    "#for i in range(0,len(all_filtered_sentence_en_train)):\n",
    "#    training_set_en.append(extract_features(all_filtered_sentence_en_train[i],word_features_en))\n",
    "training_set_en = nltk.classify.apply_features(extract_features, all_filtered_sentence_en_train)\n",
    "classifier_en = nltk.NaiveBayesClassifier.train(training_set_en)\n",
    "\n",
    "#word_features_fr = get_word_features(get_words_in_tweets(all_filtered_sentence_fr))\n",
    "all_filtered_sentence_fr_train=all_filtered_sentence_fr[:round(0.8*len(all_filtered_sentence_fr))]\n",
    "training_set_fr = nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_train)\n",
    "classifier_fr = nltk.NaiveBayesClassifier.train(training_set_fr)\n",
    "\n",
    "#word_features_de = get_word_features(get_words_in_tweets(all_filtered_sentence_de))\n",
    "all_filtered_sentence_de_train=all_filtered_sentence_de[:round(0.8*len(all_filtered_sentence_de))]\n",
    "training_set_de = nltk.classify.apply_features(extract_features, all_filtered_sentence_de_train)\n",
    "classifier_de = nltk.NaiveBayesClassifier.train(training_set_de)\n",
    "\n",
    "#word_features_it = get_word_features(get_words_in_tweets(all_filtered_sentence_it))\n",
    "all_filtered_sentence_it_train=all_filtered_sentence_it[:round(0.8*len(all_filtered_sentence_it))]\n",
    "training_set_it = nltk.classify.apply_features(extract_features, all_filtered_sentence_it_train)\n",
    "classifier_it = nltk.NaiveBayesClassifier.train(training_set_it)\n",
    "\n",
    "\n",
    "#PRINT RESULTS\n",
    "all_filtered_sentence_en_test=all_filtered_sentence_en[round(0.8*len(all_filtered_sentence_en)):]\n",
    "testing_set_en = nltk.classify.apply_features(extract_features, all_filtered_sentence_en_test)\n",
    "print(nltk.classify.accuracy(classifier_en, testing_set_en)*100)\n",
    "\n",
    "all_filtered_sentence_fr_test=all_filtered_sentence_fr[round(0.8*len(all_filtered_sentence_fr)):]\n",
    "testing_set_fr = nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_test)\n",
    "print(nltk.classify.accuracy(classifier_fr, testing_set_fr)*100)\n",
    "\n",
    "all_filtered_sentence_de_test=all_filtered_sentence_de[round(0.8*len(all_filtered_sentence_de)):]\n",
    "testing_set_de = nltk.classify.apply_features(extract_features, all_filtered_sentence_de_test)\n",
    "print(nltk.classify.accuracy(classifier_de, testing_set_de)*100)\n",
    "\n",
    "all_filtered_sentence_it_test=all_filtered_sentence_it[round(0.8*len(all_filtered_sentence_it)):]\n",
    "testing_set_it = nltk.classify.apply_features(extract_features, all_filtered_sentence_it_test)\n",
    "print(nltk.classify.accuracy(classifier_it, testing_set_it)*100)\n",
    "\n",
    "####\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from nltk.stem import PorterStemmer\n",
    "import time\n",
    "import string\n",
    "start_time = time.time()\n",
    "\n",
    "### CREATION STOP WORDS !!!   \n",
    "with open('stopwords/de') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_de = []\n",
    "for i in mylist:\n",
    "    stopwords_de.append(i)\n",
    "    \n",
    "with open('stopwords/en') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_en = []\n",
    "for i in mylist:\n",
    "    stopwords_en.append(i)\n",
    "    \n",
    "with open('stopwords/it') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_it = []\n",
    "for i in mylist:\n",
    "    stopwords_it.append(i)\n",
    "    \n",
    "with open('stopwords/fr') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_fr = []\n",
    "for i in mylist:\n",
    "    stopwords_fr.append(i)\n",
    "### FIN CREATION STOPWORDS\n",
    "    \n",
    "def filtering(stop_words, index):\n",
    "    sentence=twitterAllWith.iloc[index][\"_source\"].get('main');\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    words_filtered=[x.lower() for x in word_tokens if x not in stop_words if x not in string.punctuation if len(x)>=3 ]\n",
    "    #COULD BE ADDED IF NEEDED : if x.find('youtu.be')==-1 if x.find('twitter.com')==-1 if x.find('www')==-1 if x.find('.com')==-1\n",
    "    return words_filtered\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in get_word_features(document):\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "\"\"\"\n",
    "TwitterWithPos=[]\n",
    "TwitterWithNeg=[]\n",
    "TwitterWithNeu=[]\n",
    "TwitterWithout=[]\n",
    "twitterAllWith=[]\n",
    "all_filtered_sentence_en=[]\n",
    "all_filtered_sentence_de=[]\n",
    "all_filtered_sentence_fr=[]\n",
    "all_filtered_sentence_it=[]\n",
    "\n",
    "\n",
    "#DISSOCIATE ROW WITH/WITHOUT SENTIMENT     \n",
    "#FIND Tweets without sentiment and place it into a text file per language\n",
    "\n",
    "for i in range(0,len(twitter)):\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        TwitterWithout.append(twitter.iloc[[i]])\n",
    "    else:\n",
    "        #FIND Tweet with POSITIVE sentiment\n",
    "        if twitter._source[i].get('sentiment')=='POSITIVE':\n",
    "            TwitterWithPos.append(twitter.iloc[[i]]) \n",
    "        #FIND Tweet with NEGATIVE sentiment\n",
    "        elif twitter._source[i].get('sentiment')=='NEGATIVE':\n",
    "            TwitterWithNeg.append(twitter.iloc[[i]])\n",
    "        #FIND Tweet with NEUTRAL sentiment\n",
    "        elif twitter._source[i].get('sentiment')=='NEUTRAL':\n",
    "            TwitterWithNeu.append(twitter.iloc[[i]])\n",
    "\n",
    "#To transform into DataFrame       \n",
    "TwitterWithPos=pd.concat(TwitterWithPos, ignore_index=True)\n",
    "TwitterWithNeg=pd.concat(TwitterWithNeg, ignore_index=True)\n",
    "TwitterWithout=pd.concat(TwitterWithout, ignore_index=True)\n",
    "\n",
    "frames = [TwitterWithPos, TwitterWithNeg,TwitterWithNeu]\n",
    "twitterAllWith = pd.concat(frames)\n",
    "twitterAllWith = twitterAllWith.sample(frac=1).reset_index(drop=True) #Shuffle DF\n",
    "\"\"\"  \n",
    "TwitterWith=twitter\n",
    "for i in range(0,len(twitter)):\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        TwitterWith.drop(TwitterWith.index[[i]])\n",
    "        \n",
    "TwitterWith = TwitterWith.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "twitter['language'] = pd.Series(np.zeros(len(twitter)));\n",
    "\n",
    "for i in range(0,len(twitterAllWith)): #TO CHANGE !!! len(twitterAllWith)\n",
    "\n",
    "    #ADD COLUMN TO ADD THE LANGUAGE\n",
    "    sentence=twitterAllWith.iloc[i][\"_source\"].get('main')\n",
    "    language='None';\n",
    "    try:\n",
    "        language=detect(sentence)\n",
    "    except:\n",
    "        e=1;\n",
    "        \n",
    "    twitterAllWith.ix[i,'language']=language\n",
    "\n",
    "    if language=='en':\n",
    "        words_filtered=filtering(stopwords_en,i)\n",
    "        all_filtered_sentence_en.append((words_filtered,twitterAllWith._source[i].get('sentiment'))) \n",
    "    elif language=='fr':\n",
    "        words_filtered=filtering(stopwords_fr,i)\n",
    "        all_filtered_sentence_fr.append((words_filtered,twitterAllWith._source[i].get('sentiment')))\n",
    "    elif language=='de':\n",
    "        words_filtered=filtering(stopwords_de,i)\n",
    "        all_filtered_sentence_de.append((words_filtered,twitterAllWith._source[i].get('sentiment')))\n",
    "    elif language=='it':\n",
    "        words_filtered=filtering(stopwords_it,i)\n",
    "        all_filtered_sentence_it.append((words_filtered,twitterAllWith._source[i].get('sentiment')))                        \n",
    "\n",
    "\n",
    "#CLASSIFYING\n",
    "#word_features_en = get_word_features(get_words_in_tweets(all_filtered_sentence_en))\n",
    "all_filtered_sentence_en_train=all_filtered_sentence_en[:round(0.8*len(all_filtered_sentence_en))]\n",
    "#for i in range(0,len(all_filtered_sentence_en_train)):\n",
    "#    training_set_en.append(extract_features(all_filtered_sentence_en_train[i],word_features_en))\n",
    "training_set_en = nltk.classify.apply_features(extract_features, all_filtered_sentence_en_train)\n",
    "classifier_en = nltk.NaiveBayesClassifier.train(training_set_en)\n",
    "\n",
    "#word_features_fr = get_word_features(get_words_in_tweets(all_filtered_sentence_fr))\n",
    "all_filtered_sentence_fr_train=all_filtered_sentence_fr[:round(0.8*len(all_filtered_sentence_fr))]\n",
    "training_set_fr = nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_train)\n",
    "classifier_fr = nltk.NaiveBayesClassifier.train(training_set_fr)\n",
    "\n",
    "#word_features_de = get_word_features(get_words_in_tweets(all_filtered_sentence_de))\n",
    "all_filtered_sentence_de_train=all_filtered_sentence_de[:round(0.8*len(all_filtered_sentence_de))]\n",
    "training_set_de = nltk.classify.apply_features(extract_features, all_filtered_sentence_de_train)\n",
    "classifier_de = nltk.NaiveBayesClassifier.train(training_set_de)\n",
    "\n",
    "#word_features_it = get_word_features(get_words_in_tweets(all_filtered_sentence_it))\n",
    "all_filtered_sentence_it_train=all_filtered_sentence_it[:round(0.8*len(all_filtered_sentence_it))]\n",
    "training_set_it = nltk.classify.apply_features(extract_features, all_filtered_sentence_it_train)\n",
    "classifier_it = nltk.NaiveBayesClassifier.train(training_set_it)\n",
    "\n",
    "\n",
    "#PRINT RESULTS\n",
    "all_filtered_sentence_en_test=all_filtered_sentence_en[round(0.8*len(all_filtered_sentence_en)):]\n",
    "testing_set_en = nltk.classify.apply_features(extract_features, all_filtered_sentence_en_test)\n",
    "print(nltk.classify.accuracy(classifier_en, testing_set_en)*100)\n",
    "\n",
    "all_filtered_sentence_fr_test=all_filtered_sentence_fr[round(0.8*len(all_filtered_sentence_fr)):]\n",
    "testing_set_fr = nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_test)\n",
    "print(nltk.classify.accuracy(classifier_fr, testing_set_fr)*100)\n",
    "\n",
    "all_filtered_sentence_de_test=all_filtered_sentence_de[round(0.8*len(all_filtered_sentence_de)):]\n",
    "testing_set_de = nltk.classify.apply_features(extract_features, all_filtered_sentence_de_test)\n",
    "print(nltk.classify.accuracy(classifier_de, testing_set_de)*100)\n",
    "\n",
    "all_filtered_sentence_it_test=all_filtered_sentence_it[round(0.8*len(all_filtered_sentence_it)):]\n",
    "testing_set_it = nltk.classify.apply_features(extract_features, all_filtered_sentence_it_test)\n",
    "print(nltk.classify.accuracy(classifier_it, testing_set_it)*100)\n",
    "\n",
    "####\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature={}\n",
    "word=[\"apple\",\"cranberry\",\"sugar\"]\n",
    "for i in word:\n",
    "    feature.update({i:True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': True, 'cranberry': True, 'sugar': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE IS THE LATEST WORK!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "18\n",
      "39\n",
      "40\n",
      "47\n",
      "50\n",
      "59\n",
      "75\n",
      "80\n",
      "83\n",
      "84\n",
      "87\n",
      "90\n",
      "--- 13.506541967391968 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from nltk.stem import PorterStemmer\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "twitter=pd.read_json(\"harvest3r_twitter_data_12-01_0.json\");\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "training_set_en=[]\n",
    "\n",
    "### CREATION STOP WORDS !!!   \n",
    "with open('stopwords/de') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_de = []\n",
    "for i in mylist:\n",
    "    stopwords_de.append(i)\n",
    "    \n",
    "with open('stopwords/en') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_en = []\n",
    "for i in mylist:\n",
    "    stopwords_en.append(i)\n",
    "    \n",
    "with open('stopwords/it') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_it = []\n",
    "for i in mylist:\n",
    "    stopwords_it.append(i)\n",
    "    \n",
    "with open('stopwords/fr') as f:\n",
    "    mylist = f.read().splitlines()\n",
    "stopwords_fr = []\n",
    "for i in mylist:\n",
    "    stopwords_fr.append(i)\n",
    "### FIN CREATION STOPWORDS\n",
    "    \n",
    "all_filtered_sentence_en=[]\n",
    "all_filtered_sentence_de=[]\n",
    "all_filtered_sentence_fr=[]\n",
    "all_filtered_sentence_it=[]\n",
    "    \n",
    "def filtering(stop_words, index): #index is the index of the tweet in tweets\n",
    "    sentence=twitterWith.iloc[index][\"_source\"].get('main');\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    words_filtered=[x.lower() for x in word_tokens if x not in stop_words if x not in string.punctuation if len(x)>=3 ]\n",
    "    return words_filtered\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "def extract_features(document,word_features):\n",
    "    #document_words = set(document)\n",
    "    features = dict()\n",
    "    for word in word_features:\n",
    "        if word in document:\n",
    "            features.update({\"contains(\"+word+\"): \": True})\n",
    "        else:\n",
    "            features.update({\"contains(\"+word+\"): \": False})\n",
    "    #for word in word_features:\n",
    "    #    features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "twitterWith=twitter\n",
    "for i in range(0,100):#len(twitter)\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        print(i)\n",
    "        twitterWith.drop(twitterWith.index[[i]])\n",
    "        \n",
    "twitterWith = twitterWith.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "twitter['language'] = pd.Series(np.zeros(len(twitter)));\n",
    "\n",
    "for i in range(0,1000): #TO CHANGE !!! len(twitterWith)\n",
    "\n",
    "    #ADD COLUMN TO ADD THE LANGUAGE\n",
    "    sentence=twitterWith.iloc[i][\"_source\"].get('main')\n",
    "    language='None';\n",
    "    try:\n",
    "        language=detect(sentence)\n",
    "    except:\n",
    "        e=1;\n",
    "        \n",
    "    twitterWith.ix[i,'language']=language\n",
    "\n",
    "    if language=='en':\n",
    "        words_filtered=filtering(stopwords_en,i)\n",
    "        all_filtered_sentence_en.append((words_filtered,twitterWith._source[i].get('sentiment'))) \n",
    "    elif language=='fr':\n",
    "        words_filtered=filtering(stopwords_fr,i)\n",
    "        all_filtered_sentence_fr.append((words_filtered,twitterWith._source[i].get('sentiment')))\n",
    "    elif language=='de':\n",
    "        words_filtered=filtering(stopwords_de,i)\n",
    "        all_filtered_sentence_de.append((words_filtered,twitterWith._source[i].get('sentiment')))\n",
    "    elif language=='it':\n",
    "        words_filtered=filtering(stopwords_it,i)\n",
    "        all_filtered_sentence_it.append((words_filtered,twitterWith._source[i].get('sentiment')))                        \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.53086419753086\n",
      "94.28571428571428\n",
      "100.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "dictFeatures_test_en=dict()\n",
    "dictFeatures_train_en=dict()\n",
    "dictFeatures_test_fr=dict()\n",
    "dictFeatures_train_fr=dict()\n",
    "dictFeatures_test_de=dict()\n",
    "dictFeatures_train_de=dict()\n",
    "dictFeatures_test_it=dict()\n",
    "dictFeatures_train_it=dict()\n",
    "training_set_en=[]\n",
    "testing_set_en=[]\n",
    "training_set_fr=[]\n",
    "testing_set_fr=[]\n",
    "training_set_de=[]\n",
    "testing_set_de=[]\n",
    "training_set_it=[]\n",
    "testing_set_it=[]\n",
    "\n",
    "#CLASSIFYING\n",
    "word_features_en = get_word_features(get_words_in_tweets(all_filtered_sentence_en))\n",
    "all_filtered_sentence_en_train=all_filtered_sentence_en[:round(0.8*len(all_filtered_sentence_en))]\n",
    "for i in range(0,len(all_filtered_sentence_en_train)):\n",
    "    dictFeatures_train_en=extract_features(all_filtered_sentence_en_train[i],word_features_en)\n",
    "    training_set_en.append((dictFeatures_train_en,all_filtered_sentence_en_train[i][1]))\n",
    "#training_set_en = nltk.classify.apply_features(extract_features, all_filtered_sentence_en_train)\n",
    "classifier_en = nltk.NaiveBayesClassifier.train(training_set_en)\n",
    "\n",
    "word_features_fr = get_word_features(get_words_in_tweets(all_filtered_sentence_fr))\n",
    "all_filtered_sentence_fr_train=all_filtered_sentence_fr[:round(0.8*len(all_filtered_sentence_fr))]\n",
    "for i in range(0,len(all_filtered_sentence_fr_train)):\n",
    "    dictFeatures_train_fr=extract_features(all_filtered_sentence_fr_train[i],word_features_fr)\n",
    "    training_set_fr.append((dictFeatures_train_fr,all_filtered_sentence_fr_train[i][1]))\n",
    "#training_set_fr = nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_train)\n",
    "classifier_fr = nltk.NaiveBayesClassifier.train(training_set_fr)\n",
    "\n",
    "word_features_de = get_word_features(get_words_in_tweets(all_filtered_sentence_de))\n",
    "all_filtered_sentence_de_train=all_filtered_sentence_de[:round(0.8*len(all_filtered_sentence_de))]\n",
    "for i in range(0,len(all_filtered_sentence_de_train)):\n",
    "    dictFeatures_train_de=extract_features(all_filtered_sentence_de_train[i],word_features_de)\n",
    "    training_set_de.append((dictFeatures_train_de,all_filtered_sentence_de_train[i][1]))\n",
    "#training_set_de = nltk.classify.apply_features(extract_features, all_filtered_sentence_de_train)\n",
    "classifier_de = nltk.NaiveBayesClassifier.train(training_set_de)\n",
    "\n",
    "word_features_it = get_word_features(get_words_in_tweets(all_filtered_sentence_it))\n",
    "all_filtered_sentence_it_train=all_filtered_sentence_it[:round(0.8*len(all_filtered_sentence_it))]\n",
    "for i in range(0,len(all_filtered_sentence_it_train)):\n",
    "    dictFeatures_train_it=extract_features(all_filtered_sentence_it_train[i],word_features_it)\n",
    "    training_set_it.append((dictFeatures_train_it,all_filtered_sentence_it_train[i][1]))\n",
    "#training_set_it = nltk.classify.apply_features(extract_features, all_filtered_sentence_it_train)\n",
    "classifier_it = nltk.NaiveBayesClassifier.train(training_set_it)\n",
    "\n",
    "\n",
    "#PRINT RESULTS\n",
    "all_filtered_sentence_en_test=all_filtered_sentence_en[round(0.8*len(all_filtered_sentence_en)):]\n",
    "for i in range(0,len(all_filtered_sentence_en_test)):\n",
    "    dictFeatures_test_en=extract_features(all_filtered_sentence_en_train[i],word_features_en)\n",
    "    testing_set_en.append((dictFeatures_test_en,all_filtered_sentence_en_test[i][1]))\n",
    "#testing_set_en = nltk.classify.apply_features(extract_features, all_filtered_sentence_en_test)\n",
    "print(nltk.classify.accuracy(classifier_en, testing_set_en)*100)\n",
    "\n",
    "\n",
    "all_filtered_sentence_fr_test=all_filtered_sentence_fr[round(0.8*len(all_filtered_sentence_fr)):]\n",
    "for i in range(0,len(all_filtered_sentence_fr_test)):\n",
    "    dictFeatures_test_fr=extract_features(all_filtered_sentence_fr_train[i],word_features_fr)\n",
    "    testing_set_fr.append((dictFeatures_test_fr,all_filtered_sentence_fr_test[i][1]))\n",
    "#testing_set_fr = nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_test)\n",
    "print(nltk.classify.accuracy(classifier_fr, testing_set_fr)*100)\n",
    "\n",
    "all_filtered_sentence_de_test=all_filtered_sentence_de[round(0.8*len(all_filtered_sentence_de)):]\n",
    "for i in range(0,len(all_filtered_sentence_de_test)):\n",
    "    dictFeatures_test_de=extract_features(all_filtered_sentence_de_train[i],word_features_de)\n",
    "    testing_set_de.append((dictFeatures_test_de,all_filtered_sentence_de_test[i][1]))\n",
    "#testing_set_de = nltk.classify.apply_features(extract_features, all_filtered_sentence_de_test)\n",
    "print(nltk.classify.accuracy(classifier_de, testing_set_de)*100)\n",
    "\n",
    "all_filtered_sentence_it_test=all_filtered_sentence_it[round(0.8*len(all_filtered_sentence_it)):]\n",
    "for i in range(0,len(all_filtered_sentence_it_test)):\n",
    "    dictFeatures_test_it=extract_features(all_filtered_sentence_it_train[i],word_features_it)\n",
    "    testing_set_it.append((dictFeatures_test_it,all_filtered_sentence_it_test[i][1]))\n",
    "#testing_set_it = nltk.classify.apply_features(extract_features, all_filtered_sentence_it_test)\n",
    "print(nltk.classify.accuracy(classifier_it, testing_set_it)*100)\n",
    "\n",
    "####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
