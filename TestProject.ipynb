{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give the average sentiment PER CANTON or per city (more precise is imposible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "insta=pd.read_json(\"harvest3r_instagram_data_12-01_0.json\");\n",
    "twitter=pd.read_json(\"harvest3r_twitter_data_12-01_0.json\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2252"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news=pd.read_json(\"harvest3r_news_data_14-01_0.json\");\n",
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Null value: 4.311288556893554%\n"
     ]
    }
   ],
   "source": [
    "a=0;\n",
    "for i in range(0,len(twitter)):\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        a=a+1;\n",
    "per=a/len(twitter)*100;\n",
    "per=str(per);\n",
    "print('Percentage of Null value: ' + per + '%');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Merger (on the server)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-234942f681cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#To transform into DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mAllTwitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAllTwitter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;31m#To transform into DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mAllInstagram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAllInstagram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mAllNews\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAllNews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cyrilschmitt/anaconda/lib/python3.5/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    843\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                        copy=copy)\n\u001b[0m\u001b[1;32m    846\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cyrilschmitt/anaconda/lib/python3.5/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No objects to concatenate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import os, os.path\n",
    "files=[name for name in os.listdir('.') if os.path.isfile(name)];\n",
    "AllTwitter=[];\n",
    "AllInstagram=[];\n",
    "AllNews=[];\n",
    "a=0;\n",
    "#Add Loop \"for\" to go in different directory (january to october)\n",
    "for i in ['january','february','march','april','mai','june','july','august','september','october']:\n",
    "    files=[name for name in os.listdir('./'+i+\"/\") if os.path.isfile(name)]; #Change the path accordingly\n",
    "    for i in files:\n",
    "        if files[a].find('twitter')>0:\n",
    "            try:\n",
    "                twitter=pd.read_json(i);\n",
    "                AllTwitter.append(twitter);\n",
    "            except ValueError:\n",
    "                print (\"JSON object issue\");\n",
    "        elif files[a].find('instagram')>0:\n",
    "            try:\n",
    "                instagram=pd.read_json(i);\n",
    "                AllInstagram.append(instagram);\n",
    "            except ValueError:\n",
    "                print (\"JSON object issue\");\n",
    "        elif files[a].find('news')>0:\n",
    "            try:\n",
    "                news=pd.read_json(i);\n",
    "                AllNews.append(news);\n",
    "            except ValueError:\n",
    "                print (\"JSON object issue\");\n",
    "        a=a+1;\n",
    "\n",
    "#To transform into DataFrame\n",
    "AllTwitter=pd.concat(AllTwitter, ignore_index=True); #To transform into DataFrame\n",
    "AllInstagram=pd.concat(AllInstagram, ignore_index=True);\n",
    "AllNews=pd.concat(AllNews, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File merger (locally)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON object issue\n"
     ]
    }
   ],
   "source": [
    "import os, os.path\n",
    "files=[name for name in os.listdir('.') if os.path.isfile(name)];\n",
    "AllTwitter=[];\n",
    "AllInstagram=[];\n",
    "AllNews=[];\n",
    "a=0;\n",
    "\n",
    "files=[name for name in os.listdir('.') if os.path.isfile(name)]; \n",
    "for i in files:\n",
    "    if files[a].find('twitter')>0:\n",
    "        try:\n",
    "            twitter=pd.read_json(i);\n",
    "            AllTwitter.append(twitter);\n",
    "        except ValueError:\n",
    "            print (\"JSON object issue\");\n",
    "    elif files[a].find('instagram')>0:\n",
    "        try:\n",
    "            instagram=pd.read_json(i);\n",
    "            AllInstagram.append(instagram);\n",
    "        except ValueError:\n",
    "            print (\"JSON object issue\");\n",
    "    elif files[a].find('news')>0:\n",
    "        try:\n",
    "            news=pd.read_json(i);\n",
    "            AllNews.append(news);\n",
    "        except ValueError:\n",
    "            print (\"JSON object issue\");\n",
    "    a=a+1;\n",
    "\n",
    "#To transform into DataFrame\n",
    "AllTwitter=pd.concat(AllTwitter, ignore_index=True); #To transform into DataFrame\n",
    "AllInstagram=pd.concat(AllInstagram, ignore_index=True);\n",
    "AllNews=pd.concat(AllNews, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4567"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AllNews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to separate row with sentiment to row without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TwitterWith=[];\n",
    "TwitterWithout=[];\n",
    "for i in range(0,len(twitter)):\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        TwitterWith.append(twitter.iloc[[i]]);\n",
    "        #Put in df without sentiment\n",
    "    else:\n",
    "        TwitterWithout.append(twitter.iloc[[i]]);\n",
    "        #Put in df to do the learnning\n",
    "        #Split this table into training/testing\n",
    "        \n",
    "#To transform into DataFrame       \n",
    "TwitterWith=pd.concat(TwitterWith, ignore_index=True);\n",
    "TwitterWithout=pd.concat(TwitterWithout, ignore_index=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Baloo', 'bereit', 'Feierabend', '.', 'pic.twitter.com/XXHKCpM7nn']\n",
      "['Vierter', 'Platz', 'Wendy', 'Holdener', 'Slalom', 'Flachau', '.', 'http', ':', '//www.bote.ch', 'pic.twitter.com/1gAyQtnfrR']\n",
      "['#', 'TagderBildung', 'Kanton', 'Zürich', '.', 'Da', 'freue', 'besonders', ',', '400', 'Gymnasiasten', 'Winterthur', 'aufzutreten', '...', 'pic.twitter.com/e4L3yx4Jyq']\n",
      "['10', 'must-download', 'Google', 'Chrome', 'add-ons', 'surfing', 'web', 'http', ':', '//ti.me/1mQEDk9', 'vía', '@', 'Techland']\n",
      "['Das', 'Internet', 'Realtime', '!', 'Lasst', 'Seite', 'mal', '10', 'Minuten', 'offen', 'schaut', '!', 'Ist', 'echt', 'spannend', ',', 'abgeht', '!', 'http', ':', '//rros.ch/1OfLSeM']\n",
      "['Die', 'Strecke', 'Täsch-Zermatt', 'ab', '09.30', 'Uhr', 'wegen', 'Lawinengefahr', 'weiteres', 'gesperrt', '.']\n",
      "['@', 'broennimann', '@', 'Ugugu', '@', 'metamythos', 'AZ', '``', 'getrauen', \"''\", 'http', ':', '//m.aargauerzeitung.ch/schweiz/roger-koeppel-verharmlost-nazi-fuehrer-hermann-goering-129979655', '…']\n",
      "['Ich', 'Video', '@', 'YouTube-Playlist', 'hinzugefügt', ':', 'http', ':', '//youtu.be/PHj-dBOCpps', '?', 'a', 'Thomas', 'Anders', '-', 'I', \"'ll\", 'Be', 'Strong', '(', 'with', 'Lyrics', ')']\n",
      "['Nachhilfe', 'Chemie', 'Biologie', 'Physik', 'http', ':', '//www.nachhilfe-chemie-bio.ch/']\n",
      "['Breite', 'Allianz', 'Transithölle', ',', 'Verkehrschaos', 'Sanierungsschwindel', '#', 'Gotthard', '#', 'zweiteRöhreNEIN', 'https', ':', '//www.facebook.com/zweite.roehre.nein/videos/1703417859879209/', '?', 'theater', '…']\n",
      "['@', 'RaiBallaro', 'Mandate', 'Fornero', 'lavorare', 'catena', 'montaggio', 'Fiat', 'fino', '67', 'anni', 'poi', 'vediamo', 'cosa', 'pensa']\n",
      "['@', 'phwampfler', 'Nein', '.', 'Diskriminier', '.', 'persönlichen', 'Gründen', 'wegzuschwatzen', '(', 'müsste', 'mehr', 'zahlen', ')', 'find', \"'huere\", 'schwach', \"'\", '#', 'twoff', '@', 'JustynaGrund']\n",
      "['Guy', 'Sebastian', '-', 'Mama', 'Ai', \"n't\", 'Proud', 'ft.', '2', 'Chainz', '-', 'ListenOnRepeat', 'http', ':', '//listenonrepeat.com/watch/', '?', 'v=z_zXgXbYiHM', '#', 'Guy_Sebastian_-_Mama_Ain_t_Proud_ft._2_Chainz', '…', 'via', '@', 'listenonrepeat..']\n",
      "['Mehrere', 'Tote', 'Explosion', 'Innenstadt', '#', 'Istanbul', ':', 'http', ':', '//www.luzernerzeitung.ch/nachrichten/international/international-sda/Tote-bei-Explosion-in-Touristenviertel', ';', 'art46446,664523', '…']\n"
     ]
    }
   ],
   "source": [
    "#Parsing each Tweet into word\n",
    "for i in range(0,20): #len(twitter) instead of 20\n",
    "    sentence=twitter.iloc[i][\"_source\"];\n",
    "    sentence=sentence.get('main');\n",
    "    \n",
    "    # 1) Get Language of tweet!\n",
    "    \n",
    "    language=detect(sentence);\n",
    "    \n",
    "    if language=='en' or language=='de' or language=='it': #Add the other of language we want manually!\n",
    "        \n",
    "        langDict = {'en': 'english', 'de': 'german', 'it':'italian'} #Add accordingly\n",
    "\n",
    "        # 2) Delete the stop_words (useless ones)\n",
    "        stop_words = set(stopwords.words(langDict[language])) #Replace with \"English\", \"German\", \"French\", \"Arabic\" or whatever.\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "        filtered_sentence = []\n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w);\n",
    "        print(filtered_sentence);\n",
    "        \n",
    "        #For lexical understanding, might want to use WordNet in english, Germanet for german and MultiWordNet for Italian or Spanish\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
