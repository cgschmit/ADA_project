{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give the average sentiment PER CANTON or per city (more precise is imposible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitter=pd.read_json(\"harvest3r_twitter_data_12-01_0.json\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Merger (on the server)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "files=[name for name in os.listdir('.') if os.path.isfile(name)];\n",
    "AllTwitter=[];\n",
    "AllInstagram=[];\n",
    "AllNews=[];\n",
    "a=0;\n",
    "#Add Loop \"for\" to go in different directory (january to october)\n",
    "for i in ['january','february','march','april','mai','june','july','august','september','october']:\n",
    "    files=[name for name in os.listdir('./'+i+\"/\") if os.path.isfile(name)]; #Change the path accordingly\n",
    "    for i in files:\n",
    "        if files[a].find('twitter')>0:\n",
    "            try:\n",
    "                twitter=pd.read_json(i);\n",
    "                AllTwitter.append(twitter);\n",
    "            except ValueError:\n",
    "                print (\"JSON object issue\");\n",
    "        elif files[a].find('instagram')>0:\n",
    "            try:\n",
    "                instagram=pd.read_json(i);\n",
    "                AllInstagram.append(instagram);\n",
    "            except ValueError:\n",
    "                print (\"JSON object issue\");\n",
    "        elif files[a].find('news')>0:\n",
    "            try:\n",
    "                news=pd.read_json(i);\n",
    "                AllNews.append(news);\n",
    "            except ValueError:\n",
    "                print (\"JSON object issue\");\n",
    "        a=a+1;\n",
    "\n",
    "#To transform into DataFrame\n",
    "AllTwitter=pd.concat(AllTwitter, ignore_index=True); #To transform into DataFrame\n",
    "AllInstagram=pd.concat(AllInstagram, ignore_index=True);\n",
    "AllNews=pd.concat(AllNews, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File merger (locally)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON object issue\n"
     ]
    }
   ],
   "source": [
    "import os, os.path\n",
    "files=[name for name in os.listdir('.') if os.path.isfile(name)];\n",
    "AllTwitter=[];\n",
    "AllInstagram=[];\n",
    "AllNews=[];\n",
    "a=0;\n",
    "\n",
    "files=[name for name in os.listdir('.') if os.path.isfile(name)]; \n",
    "for i in files:\n",
    "    if files[a].find('twitter')>0:\n",
    "        try:\n",
    "            twitter=pd.read_json(i);\n",
    "            AllTwitter.append(twitter);\n",
    "        except ValueError:\n",
    "            print (\"JSON object issue\");\n",
    "    elif files[a].find('instagram')>0:\n",
    "        try:\n",
    "            instagram=pd.read_json(i);\n",
    "            AllInstagram.append(instagram);\n",
    "        except ValueError:\n",
    "            print (\"JSON object issue\");\n",
    "    elif files[a].find('news')>0:\n",
    "        try:\n",
    "            news=pd.read_json(i);\n",
    "            AllNews.append(news);\n",
    "        except ValueError:\n",
    "            print (\"JSON object issue\");\n",
    "    a=a+1;\n",
    "\n",
    "#To transform into DataFrame\n",
    "AllTwitter=pd.concat(AllTwitter, ignore_index=True); #To transform into DataFrame\n",
    "AllInstagram=pd.concat(AllInstagram, ignore_index=True);\n",
    "AllNews=pd.concat(AllNews, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to separate row with sentiment to row without + Write in positive.txt and negative.txt + delete stop_words (useless ones)\n",
    "\n",
    "\n",
    "# I cannot concatenate, don't know why !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def deleteContent(pfile):\n",
    "    pfile.seek(0)\n",
    "    pfile.truncate()\n",
    "\n",
    "additional_stopwords=['.',',','/','(',')','[',']',':','@','\\'','?','!','\\\"',';','&']\n",
    "TwitterWithPos=[]\n",
    "TwitterWithNeg=[]\n",
    "TwitterWithout=[]\n",
    "fPos = open('positive.txt', 'r+')\n",
    "fNeg = open('negative.txt', 'r+')\n",
    "fNeu = open('neutral.txt', 'r+')\n",
    "#English\n",
    "#fPosEN = open('positive.txt', 'r+')\n",
    "#fNegEN = open('negative.txt', 'r+')\n",
    "#fNeuEN = open('neutral.txt', 'r+')\n",
    "#French\n",
    "#fPosFR = open('positive.txt', 'r+')\n",
    "#fNegFR = open('negative.txt', 'r+')\n",
    "#fNeuFR = open('neutral.txt', 'r+')\n",
    "#German\n",
    "#fPosDE = open('positive.txt', 'r+')\n",
    "#fNegDE = open('negative.txt', 'r+')\n",
    "#fNeuDE = open('neutral.txt', 'r+')\n",
    "#Italian\n",
    "#fPosIT = open('positive.txt', 'r+')\n",
    "#fNegIT = open('negative.txt', 'r+')\n",
    "#fNeuIT = open('neutral.txt', 'r+')\n",
    "\n",
    "deleteContent(fPos)\n",
    "deleteContent(fNeg)\n",
    "deleteContent(fNeu)\n",
    "twitter['language'] = pd.Series(np.zeros(len(twitter)));\n",
    "\n",
    "for i in range(0,400): #TO CHANGE !!! len(twitter)\n",
    "    sentence=twitter.iloc[i][\"_source\"].get('main');\n",
    "    language='None';\n",
    "    try:\n",
    "        language=detect(sentence); \n",
    "    except:\n",
    "        e=1;\n",
    "    twitter.ix[i,'language']=language;\n",
    "    #FIND the language of the current Tweet\n",
    "    if language=='en' or language=='de' or language=='it' or language=='fr': \n",
    "        langDict = {'en': 'english', 'de': 'german', 'it':'italian', 'fr':'french'} \n",
    "        stop_words = set(stopwords.words(langDict[language])) \n",
    "        word_tokens = word_tokenize(sentence)\n",
    "        filtered_sentence = []\n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words and w not in additional_stopwords and len(w)>=3:\n",
    "                filtered_sentence.append(w);\n",
    "    #FIND Tweets without sentiment and place it into a text file per language\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        TwitterWithout.append(twitter.iloc[[i]])\n",
    "        try:\n",
    "            filtered_sentence=' '.join(filtered_sentence)\n",
    "        except ValueError:\n",
    "            print('One-word Tweet')  \n",
    "        fNeu.write(str(filtered_sentence)+\"\\n\");\n",
    "    #FIND Tweets with sentiment and place it into a text file per language\n",
    "    else:\n",
    "        #FIND Tweet with POSITIVE sentiment\n",
    "        if twitter._source[i].get('sentiment')=='POSITIVE':\n",
    "            TwitterWithPos.append(twitter.iloc[[i]])\n",
    "            try:\n",
    "                filtered_sentence=' '.join(filtered_sentence)\n",
    "            except ValueError:\n",
    "                print('One-word Tweet') \n",
    "            fPos.write(str(filtered_sentence)+\"\\n\");\n",
    "        #FIND Tweet with NEGATIVE sentiment\n",
    "        elif twitter._source[i].get('sentiment')=='NEGATIVE':\n",
    "            TwitterWithNeg.append(twitter.iloc[[i]])\n",
    "            try:\n",
    "                filtered_sentence=' '.join(filtered_sentence)\n",
    "            except ValueError:\n",
    "                print('One-word Tweet') \n",
    "            fNeg.write(str(filtered_sentence)+\"\\n\");\n",
    "\n",
    "#To transform into DataFrame       \n",
    "TwitterWithPos=pd.concat(TwitterWithPos, ignore_index=True)\n",
    "TwitterWithNeg=pd.concat(TwitterWithNeg, ignore_index=True)\n",
    "TwitterWithout=pd.concat(TwitterWithout, ignore_index=True)\n",
    "\n",
    "fPos.close()\n",
    "fNeg.close()\n",
    "fNeu.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Last work to classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last problem still occuring:\n",
    "\n",
    "1) I have no clue how to get all the word features in \"extract_features\" function. Problem linked to the fact we're using: nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_train) \n",
    "    \n",
    "    nltk.classify.apply_features(function, toks)\n",
    "    \n",
    "    [function(tok) for tok in toks]\n",
    "    \n",
    "2) Problem to compute the accuracy. It worked at some point but could'nt fix it back with all the changes I added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-288ffeaed518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0mall_filtered_sentence_en_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_filtered_sentence_en\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_filtered_sentence_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mtraining_set_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_filtered_sentence_en_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0mclassifier_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m#word_features_fr = get_word_features(get_words_in_tweets(all_filtered_sentence_fr))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cyrilschmitt/anaconda/lib/python3.5/site-packages/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Count up how many times each feature value occurred, given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# the label and featurename.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mlabel_freqdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cyrilschmitt/anaconda/lib/python3.5/site-packages/nltk/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lists\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cyrilschmitt/anaconda/lib/python3.5/site-packages/nltk/classify/util.py\u001b[0m in \u001b[0;36mlazy_func\u001b[0;34m(labeled_token)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlazy_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeature_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLazyMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-288ffeaed518>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(document)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdocument_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'contains(%s)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_features' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from nltk.stem import PorterStemmer\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "def filtering():\n",
    "    sentence=twitter.iloc[i][\"_source\"].get('main');\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    words_filtered=[x.lower() for x in word_tokens if x not in stop_words if x not in additional_stopwords if len(x)>=3]\n",
    "    return words_filtered\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "\n",
    "additional_stopwords=['.',',','/','(',')','[',']',':','@','\\'','?','!','\\\"',';','&']\n",
    "TwitterWithPos=[]\n",
    "TwitterWithNeg=[]\n",
    "TwitterWithout=[]\n",
    "all_filtered_sentence_en=[]\n",
    "all_filtered_sentence_de=[]\n",
    "all_filtered_sentence_fr=[]\n",
    "all_filtered_sentence_it=[]\n",
    "\n",
    "twitter['language'] = pd.Series(np.zeros(len(twitter)));\n",
    "\n",
    "for i in range(0,400): #TO CHANGE !!! len(twitter)\n",
    "    \n",
    "    #ADD COLUMN TO ADD THE LANGUAGE\n",
    "    sentence=twitter.iloc[i][\"_source\"].get('main')\n",
    "    language='None';\n",
    "    try:\n",
    "        language=detect(sentence)\n",
    "    except:\n",
    "        e=1;\n",
    "        \n",
    "    twitter.ix[i,'language']=language\n",
    "    \n",
    "    #FIND the language of the current Tweet and create tuple (words of sentence, sentiment)\n",
    "    if language=='en' or language=='de' or language=='it' or language=='fr': \n",
    "        langDict = {'en': 'english', 'de': 'german', 'it':'italian', 'fr':'french'} \n",
    "        stop_words = set(stopwords.words(langDict[language])) \n",
    "        \n",
    "        if language=='en':\n",
    "            words_filtered=filtering()\n",
    "            all_filtered_sentence_en.append((words_filtered,twitter._source[i].get('sentiment'))) \n",
    "         \n",
    "        elif language=='fr':\n",
    "            words_filtered=filtering()\n",
    "            all_filtered_sentence_fr.append((words_filtered,twitter._source[i].get('sentiment')))\n",
    "\n",
    "        elif language=='de':\n",
    "            words_filtered=filtering()\n",
    "            all_filtered_sentence_de.append((words_filtered,twitter._source[i].get('sentiment')))\n",
    "            \n",
    "        elif language=='it':\n",
    "            words_filtered=filtering()\n",
    "            all_filtered_sentence_it.append((words_filtered,twitter._source[i].get('sentiment')))                        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #DISSOCIATE ROW WITH/WITHOU SENTIMENT     \n",
    "    #FIND Tweets without sentiment and place it into a text file per language\n",
    "    if pd.isnull(twitter._source[i].get('sentiment')):\n",
    "        TwitterWithout.append(twitter.iloc[[i]])\n",
    "    #FIND Tweets with sentiment and place it into a text file per language\n",
    "    else:\n",
    "        #FIND Tweet with POSITIVE sentiment\n",
    "        if twitter._source[i].get('sentiment')=='POSITIVE':\n",
    "            TwitterWithPos.append(twitter.iloc[[i]]) \n",
    "        #FIND Tweet with NEGATIVE sentiment\n",
    "        elif twitter._source[i].get('sentiment')=='NEGATIVE':\n",
    "            TwitterWithNeg.append(twitter.iloc[[i]])\n",
    "\n",
    "#CLASSIFYING\n",
    "\n",
    "#word_features_en = get_word_features(get_words_in_tweets(all_filtered_sentence_en))\n",
    "all_filtered_sentence_en_train=all_filtered_sentence_en[:round(0.8*len(all_filtered_sentence_en))]\n",
    "training_set_en = nltk.classify.apply_features(extract_features, all_filtered_sentence_en_train)\n",
    "classifier_en = nltk.NaiveBayesClassifier.train(training_set_en)\n",
    "\n",
    "#word_features_fr = get_word_features(get_words_in_tweets(all_filtered_sentence_fr))\n",
    "all_filtered_sentence_fr_train=all_filtered_sentence_fr[:round(0.8*len(all_filtered_sentence_fr))]\n",
    "training_set_fr = nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_train)\n",
    "classifier_fr = nltk.NaiveBayesClassifier.train(training_set_fr)\n",
    "\n",
    "#word_features_de = get_word_features(get_words_in_tweets(all_filtered_sentence_de))\n",
    "all_filtered_sentence_de_train=all_filtered_sentence_de[:round(0.8*len(all_filtered_sentence_de))]\n",
    "training_set_de = nltk.classify.apply_features(extract_features, all_filtered_sentence_de_train)\n",
    "classifier_de = nltk.NaiveBayesClassifier.train(training_set_de)\n",
    "\n",
    "#word_features_it = get_word_features(get_words_in_tweets(all_filtered_sentence_it))\n",
    "all_filtered_sentence_it_train=all_filtered_sentence_it[:round(0.8*len(all_filtered_sentence_it))]\n",
    "training_set_it = nltk.classify.apply_features(extract_features, all_filtered_sentence_it_train)\n",
    "classifier_it = nltk.NaiveBayesClassifier.train(training_set_it)\n",
    "\n",
    "\n",
    "#PRINT RESULTS\n",
    "all_filtered_sentence_en_test=all_filtered_sentence_en[round(0.8*len(all_filtered_sentence_en)):]\n",
    "testing_set_en = nltk.classify.apply_features(extract_features, all_filtered_sentence_en_test)\n",
    "print(nltk.classify.accuracy(classifier_en, testing_set_en)*100)\n",
    "\n",
    "all_filtered_sentence_fr_test=all_filtered_sentence_fr[round(0.8*len(all_filtered_sentence_fr)):]\n",
    "testing_set_fr = nltk.classify.apply_features(extract_features, all_filtered_sentence_fr_test)\n",
    "print(nltk.classify.accuracy(classifier_fr, testing_set_fr)*100)\n",
    "\n",
    "all_filtered_sentence_de_test=all_filtered_sentence_de[round(0.8*len(all_filtered_sentence_de)):]\n",
    "testing_set_de = nltk.classify.apply_features(extract_features, all_filtered_sentence_de_test)\n",
    "print(nltk.classify.accuracy(classifier_de, testing_set_de)*100)\n",
    "\n",
    "all_filtered_sentence_it_test=all_filtered_sentence_it[round(0.8*len(all_filtered_sentence_it)):]\n",
    "testing_set_it = nltk.classify.apply_features(extract_features, all_filtered_sentence_it_test)\n",
    "print(nltk.classify.accuracy(classifier_it, testing_set_it)*100)\n",
    "\n",
    "#########\n",
    "\n",
    "#To transform into DataFrame       \n",
    "TwitterWithPos=pd.concat(TwitterWithPos, ignore_index=True)\n",
    "TwitterWithNeg=pd.concat(TwitterWithNeg, ignore_index=True)\n",
    "TwitterWithout=pd.concat(TwitterWithout, ignore_index=True)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
